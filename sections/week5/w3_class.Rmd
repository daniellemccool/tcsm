## In-Class Exercises

This week, we will wrap up our re-analysis of the Kestilä (2006) results. During 
this practical, you will conduct a CFA of the *Trust in Politics* items and 
compare the results to those obtained from your previous EFA- and PCA-based 
replications of Kestilä (2006).

---

###

Load the ESS data.

- The relevant data are contained in the *ess_round1.rds* file.
    - This file is in R Data Set (RDS) format.
    - The dataset is already stored as a data frame with the processing and 
    cleaning that you should have done for previous practicals completed.
    - Check the documentation for the `readRDS()` function to see how you can 
    load data stored in an RDS file.

<details>
  <summary>Click for explanation</summary>
  
```{r, echo = FALSE}
ess <- readRDS(paste0(dataDir, "ess_round1.rds"))
```

```{r, eval = FALSE}
ess <- readRDS("ess_round1.rds")
```

</details>

---

Although you may have settled on any number of EFA solutions during the 
[Week 2 In-Class Exercises](in-class-exercises-1.html), we are going to base the 
following CFA on a three-factor model of *Trust in Politics* similar to the 
original PCA results from Kastelä (2006).

*Note:* Unless otherwise specified, all following questions refer to the *Trust 
in Politics* items. We will not consider the *Attitudes toward Immigration* 
items in these exercises.

---

### {#cfaSyntax}

Define the `lavaan` model syntax for the CFA implied by the three-factor EFA 
solution you found in the Week 2 In-Class Exercises.

- Covary the three latent factors.
- Do not specify any mean structure.
- As in \@ref(syntax), save this model syntax as an object in your environment.

<details>
  <summary>Click for explanation</summary>

We don't have to specify the latent covariances in the model syntax, we can tell 
`lavaan` to estimate all latent covariances when we fit the model.

```{r}
mod_3f <- '
politicians  =~ pltcare + pltinvt + trstplt
satisfaction =~ stfeco  + stfgov  + stfdem + stfedu  + stfhlth
institutions =~ trstlgl + trstplc + trstun + trstprl
'
```

</details>

---

###

Estimate the CFA model you defined above, and summarize the results.

- Use the `lavaan::cfa()` function to estimate the model.
- Use the default settings for the `cfa()` function.
- Request the model fit statistics with the summary by supplying the 
`fit.measures = TRUE` argument to `summary()`.
- Request the standardized parameter estimates with the summary by supplying the 
`standardized = TRUE` argument to `summary()`.

Check the results, and answer the following questions:

- Does the model fit the data well?
- How are the latent variances and covariances specified when using the default 
settings?
- How is the model identified when using the default settings?

<details>
  <summary>Click for explanation</summary>


```{r}
## Load the lavaan package:
library(lavaan)

## Estimate the CFA model:
fit_3f <- cfa(mod_3f, data = ess)

## Summarize the fitted model:
summary(fit_3f, fit.measures = TRUE, standardized = TRUE)
```

No, the model does not seem to fit the data well.

- The SRMR looks good, but one good looking fit statistic is not enough.
- The RMSEA, TLI, and CFI are all in the "unacceptable" range.
- The $\chi^2$ is highly significant, but we don't care.

The `cfa()` function is just a wrapper for the `lavaan()` function with several 
options set at the defaults you would want for a standard CFA. 

- By default:
    - All latent variances and covariances are freely estimated (due to the 
    argument `auto.cov.lv.x = TRUE`)
    - The model is identified by fixing the first factor loading of each factor 
    to 1 (due to the argument `auto.fix.first = TRUE`)

To see a full list of the (many) options you can specify to tweak the behavior
of `lavaan` estimation functions run `?lavOptions`.

</details>

---

Now, we will consider a couple of alternative factor structures for the *Trust 
in Politics* CFA. First, we will go extremely simple by estimating a one-factor 
model wherein all *Trust* items are explained by a single latent variable.

---

###

Define the `lavaan` model syntax for a one-factor model of the *Trust* items.

- Save this syntax as an object in your environment.

<details>
  <summary>Click for explanation</summary>

```{r}
mod_1f <- '
political_trust =~ 
  pltcare + 
  pltinvt + 
  trstprl + 
  trstplt + 
  stfeco + 
  stfgov + 
  stfdem + 
  stfedu + 
  stfhlth + 
  trstlgl + 
  trstplc + 
  trstun + 
  trstep
'
```

</details>

---

###

Estimate the one-factor model, and summarize the results.

- Does this model appear to fit better or worse than the three-factor model?

*Notes:* 

- We can also conduct a formal significance test for the difference between two 
$\chi^2$ values, but we won't introduce those ideas until Week 5.
- You can use the `lavaan::fitMeasures()` function to extract only the model fit 
information from a fitted `lavaan` object.

<details>
  <summary>Click for explanation</summary>

```{r}
## Estimate the one factor model:
fit_1f <- cfa(mod_1f, data = ess)

## Summarize the results:
summary(fit_1f, fit.measures = TRUE)


## Compare fit statistics:
fitMeasures(fit_3f)
fitMeasures(fit_1f)
```

The one-factor model definitely seems to fit worse than the three-factor model.

</details>

---

A second order CFA model is another way of representing the latent structure 
underlying a set of items. As you read in Byrne (2005), however, the second 
order CFA is only appropriate in certain circumstances.

---

###

Given the CFA results above, would a second order CFA be appropriate for the 
*Trust* data? Why or why not?

<details>
  <summary>Click for explanation</summary>

Yes, a second order CFA model is a theoretically appropriate representation of 
the *Trust* items. 

- The first order latent variables in the three-factor model are all 
significantly correlated.
- The first order latent variables in the three-factor model seem to tap 
different aspects of some single underlying construct. 

</details>

---

###

Define the `lavaan` model syntax for a second-order CFA model of the *Trust* items.

- Use the three factors defined in \@ref(cfaSyntax) as the first order factors.
    
<details>
  <summary>Click for explanation</summary>

To define the second order factor, we use the same syntactic conventions that we
employ to define a first order factor. The only differences is that the 
"indicators" of the second order factor (i.e., the variables listed on the RHS 
of the `=~` operator) are previously defined first order latent variables.

```{r}
mod_2nd <- '
politicians  =~ pltcare + pltinvt + trstplt
satisfaction =~ stfeco  + stfgov  + stfdem + stfedu  + stfhlth
institutions =~ trstlgl + trstplc + trstun + trstprl

trust =~ politicians + satisfaction + institutions
'
```

</details>

---

###

Estimate the second order CFA model, and summarize the results.

- Does this model fit better or worse than the three-factor model?
- Is this model more or less complex than the three-factor model?
    - What information can you use to quantify this difference in complexity?

<details>
  <summary>Click for explanation</summary>

We don't have to do anything special here. We can estimate and summarize the 
second order CFA exactly as we did the first order CFA.

```{r}
fit_2nd <- cfa(mod_2nd, data = ess)
summary(fit_2nd, fit.measures = TRUE, standardized = TRUE)

## Compare fit between the first and second order models:
fitMeasures(fit_3f)
fitMeasures(fit_2nd)
```

You should quickly notice something strange about the model fit statistics 
compared above. If you don't see it, consider the following:

```{r}
fitMeasures(fit_3f) - fitMeasures(fit_2nd)
```

The two models produce identical fit statistics! We also see that the degrees of 
freedom are identical between the two models. Hence, the two models have equal
complexity.

This result taps into a critical idea in statistical modeling, namely, *model 
equivalency*. It turns out the two models we're comparing here are *equivalent* 
in the sense that they are statistically indistinguishable representations of 
the data.

Since this is a very important idea, I want to spend some time discussing it in 
person. So, spend some time between now and the next lecture session thinking 
about the implications of this model equivalence. Specifically, consider the 
following questions:

- What do we mean when we say that these two models are *equivalent*?
- How is it possible for these two models to be *equivalent* when one contains 
an additional latent variable?
- Why are the degrees of freedom equal for these two models?
- Why are the fit statistics equal for these two models?
- How can we go about picking the "better" of these two models?

We'll take some time to discuss these ideas in the next lecture session.

</details>

---

The above represents the extent to which we will use CFA to replicate the 
EFA/PCA components of the Kastelä (2006) analysis. We will now use the latent 
variables defined above to revisit the regression analysis from Kastelä (2006).

Note that we will now be specifying regression models involving latent variables,
so this is actually your first taste of full structural equation modeling (SEM).
Enjoy!

---

Unfortunately, we need to do a little data processing before we can fit the
regression model. At the moment, `lavaan` will not automatically convert a factor 
variable into dummy codes. So, we need to create explicit dummy codes for the 
two factors we'll use as predictors in our regression analysis: *sex*  and 
*political orientation*.  

---

###

Convert the *sex* and *political interest* factors into dummy codes.

<details>
  <summary>Click for explanation</summary>

In R, we have several ways of converting a factor into an appropriate set of 
dummy codes.

- We could use the `dplyr::recode()` function as we did last week.
- We can use the `model.matrix()` function to define a design matrix based on 
the inherent contrast attribute of the factor.
    - Missing data will cause problems here.
- We can us `as.numeric()` to revert the factor to its underlying numeric 
representation {Male = 1, Female = 2} and use arithmetic to convert {1, 2} 
$\rightarrow$ {0, 1}.

When our factor only has two levels, though, the `ifelse()` function is the 
simplest way.

```{r}
## Create a dummy code by broadcasting a logical test on the factor levels:
ess$female <- ifelse(ess$sex == "Female", 1, 0)

## The same for political orientation:
ess$hi_pol_interest <- ifelse(ess$polintr_bin == "High Interest", 1, 0)

## Check the results:
with(ess, table(dummy = female, factor = sex))
with(ess, table(dummy = hi_pol_interest, factor = polintr_bin))
```

---

###

Finally, subset the data to only Finnish participants.

<details>
  <summary>Click for explanation</summary>

```{r}
ess_fin <- ess[ess$cntry == "Finland", ]
```

</details>

---

We are now ready to estimate our latent regression model. Specifically, we want
to implement the following regression as an SEM in `lavaan`.

\[
Y_{trust\_inst} = \beta_1 X_{age} + \beta_2 X_{female} + \beta_3 X_{edu\_years} +
                  \beta_4 X_{hi\_pol\_interest} + \beta_5 X_{lrscale} + 
                  \varepsilon_Y
\]

---

###

Define the `lavaan` model syntax for the regression shown above.

- Use the definition of the `institutions` factor from \@ref(cfaSyntax) to 
define the DV.

*Hint:* You can simply copy the line of syntax that defines the latent factor 
and add a line to define the latent regression model.

<details>
<summary>Click for explanation</summary>

```{r}
mod_sem <- '
## Define the latent DV:
institutions =~ trstlgl + trstplc + trstun

## Specify the structural relations:
institutions ~ female + age + eduyrs + hi_pol_interest + lrscale
'
```

</details>

---

###

Estimate the SEM, and summarize the results.

- Fit the model to the processed Finnish subsample from above.
- Estimate the model using `lavaan::sem()`.
- Use the default settings in the `sem()` function.
- Request the standardized parameter estimates with the summary.
- Request the $R^2$ estimates with the summary by supplying the `rsquare = TRUE` 
argument to `summary()`.

<details>
  <summary>Click for explanation</summary>

```{r}
## Fit the SEM:
fit_sem <- sem(mod_sem, data = ess_fin)

## Summarize the results:
summary(fit_sem, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)
```

</details>

---

###

Finally, we will rerun the latent regression model from above as a path model 
with the factor score for *Trust in Institutions* as the DV.

- Add the *Trust in Institutions* factor score that you estimated in 
\@ref(factorScores) to the ESS data.
    - If you did not save the factors scores last week, you'll need to rerun the 
    relevant EFA.
    - Don't forget to subset the data to the Finish participants before fitting
    the path model.
- Rerun the above SEM with the EFA-derived *Trust in Institutions* factor score
taking the place of the analagous latent variable as DV.
    - Request the standardized parameter estimates with the summary.
    - Request the $R^2$ estimates with the summary.

<details>
  <summary>Click for explanation</summary>

First, we'll quickly reproduce the *Trust in Institutions* factor score that we
estimated last week.

- Note that `psych::fa()` returns the factor scores in a different order than it
did for the Week 2 analyses. 

```{r}
## Load the psych library:
library(psych)

## Rerun the three-factor EFA from last week:
fit_efa <- fa(ess[7:19], 
              nfactors = 3,          
              rotate   = "promax",   
              scores   = "Bartlett")

## View the factor loadings:
print(fit_efa$loadings, cut = 0.3)

## Reproduce the factor score from last week:
ess$trust_inst_efa <- fit_efa$scores[ , 1]

## Subset the data again:
ess_fin <- ess[ess$cntry == "Finland", ]
```

Now, we'll rerun our regression as a path analysis with the EFA-derived factor
score as DV.

```{r}
## Define the model syntax for the path analysis:
mod_pa <- 'trust_inst_efa ~ female + age + eduyrs + hi_pol_interest + lrscale'

## Estimate the path model:
fit_pa <- sem(mod_pa, data = ess_fin)

## Summarize the results:
summary(fit_pa, fit.measures = TRUE, standardized = TRUE, rsquare = TRUE)
```

</details>

---

###

Compare the results from the path analysis to the SEM-based results.

- Does it matter whether we use a latent variable or a factor score to define 
the DV?

*Hint:* When comparing parameter estimates, use the fully standardized estimates 
(i.e., the values in the column labeled `Std.all`).

<details>
  <summary>Click for explanation</summary>

First, we'll source a script that defines a bunch of convenience functions. One
of these functions, `partSummary()`, will allow us to print only the interesting
pieces of the model summary.

```{r, eval = FALSE}
## Source a script of convenience function definitions:
source("supportFunctions.R")
```

```{r, echo = FALSE}
source(paste0(codeDir, "supportFunctions.R"))
```

```{r}
## The partSummary() function requires the dplyr package:
library(dplyr)
```

Now, we'll compare the results. Specifically, we're interested in differences in
the regression coefficients and the $R^2$.

```{r}
## View the regression estimates from the SEM:
partSummary(fit_sem, 8, standardized = TRUE)

## View the regression estimates from the path analysis:
partSummary(fit_pa, 7, standardized = TRUE)

## View the R-squared estimates from the SEM:
partSummary(fit_sem, 10, rsquare = TRUE)

## View the R-squared estimate from the path analysis:
partSummary(fit_pa, 9, rsquare = TRUE)
```

```{r, echo = FALSE}
r2_sem <- summary(fit_sem, rsquare = TRUE)$pe %>% 
  filter(op == "r2", rhs == "institutions") %>% 
  select(est) %>% 
  as.numeric()
r2_pa <- summary(fit_pa, rsquare = TRUE)$pe %>% 
  filter(op == "r2") %>% 
  select(est) %>% 
  as.numeric()
```

It certainly looks like the way we define the DV has a meaningful impact. The 
patterns of significance differ between the two sets of regression slopes, and
the $R^2$ is `r 100 * ((r2_sem / r2_pa) - 1) %>% round(3)`% larger in the SEM.

</details>

---

End of In-Class Exercises

---
