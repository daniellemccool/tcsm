[["index.html", "Theory Construction and Statistical Modeling Course Information", " Theory Construction and Statistical Modeling Kyle M. Lang Last updated: 2022-09-06 Course Information In order to test a theory, we must express the theory as a statistical model and then test this model on quantitative (numeric) data. This course uses existing tutorial datasets to practice the process of translating verbal theories into testable statistical models. If you are interested in the methods of acquiring high quality data to test your own theory, we recommend following the course Conducting a Survey which is taught from November to January. In this course we will use datasets from different disciplines within the social sciences (educational sciences, psychology, and sociology) to explain and illustrate theories and practices that are used in all social science disciplines to statistically model social science theories. Most information about the course is available in this GitBook. Course-related communication will be through https://uu.blackboard.nl (Log in with your student ID and password). "],["acknowledgement.html", "Acknowledgement", " Acknowledgement This course was originally developed by dr. Caspar van Lissa. Indeed, you will still see Caspar in the lecture recordings. I (dr. Kyle M. Lang) have modified Caspar’s original materials and take full responsibility for any errors or inaccuracies introduced through these modifications. Credit for any particularly effective piece of pedagogy should probably go to Caspar. You can view the original version of this course here on Caspar’s GitHub page. "],["instructors.html", "Instructors", " Instructors Coordinator: dr. Kyle M. Lang Lectures: dr. Kyle M. Lang Practicals: Rianne Kraakman Daniëlle Remmerswaal Laura Jochim "],["course-overview.html", "Course overview", " Course overview This course comprises three parts: Factor analysis: You will learn different ways of defining and estimating unobserved constructs. Path analysis: You will learn how to conduct regressions and ANOVAs as structural equation models with observed variables. Full structural equation modeling: You will combine the first two topics by estimating path models using latent variables. Each of these three themes will be evaluated with a separate assignment. Your course grade will be based on the weighted average of these three assignment grades. Schedule Course Week Calendar Week Topic Assignment 0 36 Preperation, Working w/ R 1 37 Introduction to statistical modeling 2 38 Exploratory factor analyis (EFA) 3 39 Confirmatory factor analysis (CFA) 4 40 N/A Project 1 Due 5 41 General linear model (GLM), Structural equation modeling (SEM) 6 42 Mediation 7 43 Moderation Project 2 Due 8 44 Full SEM 9 45 N/A Project 3 Due "],["learning-goals.html", "Learning goals", " Learning goals In this course you will learn how to translate a social scientific theory into a statistical model, how to analyze your data with these models, and how to interpret and report your results following APA standards. After completing the course, you will be able to: Translate a verbal theory into a conceptual model, and translate a conceptual model into a statistical model. Independently analyze data using the free, open-source statistical software R. Apply a latent variable model to a real-life problem wherein the observed variables are only indirect indicators of an unobserved construct. Use a path model to represent the hypothesized causal relations among several variables, including relationships such as mediation and moderation. Explain to a fellow student how structural equation modeling combines latent variable models with path models and the benefits of doing so. Reflect critically on the decisions involved in defining and estimating structural equation models. "],["resources.html", "Resources", " Resources Literature You do not need a separate book for this course! Most of the information is contained within this GitBook and the course readings (which you will be able to access via links in this GitBook). All literature is freely available online, as long as you are logging in from within the UU-domain (i.e., from the UU campus or through an appropriate VPN). All readings are linked in this GitBook via their DOIs. If you run into any trouble accessing a given article, searching for the title using Google Scholar or the University Library will probably due the trick. Software You will do all of your statistical analyses with the statistical programming language/environment R and the add-on package lavaan. If you want to expand your learning, you can follow this excellent lavaan tutorial. Doing so is entirely optional, though. "],["reading-questions.html", "Reading questions", " Reading questions Along with every article, we will provide reading questions. You will not be graded on the reading questions, but it is important to prepare the reading questions before every lecture. The reading questions serve several important purposes: Provide relevant background knowledge for the lecture Help you recognize and understand the key terms and concepts Make you aware of important publications that shaped the field Help you extract the relevant insights from the literature "],["weekly-preparation.html", "Weekly preparation", " Weekly preparation Before every class meeting (both lectures and practicals) you need to do the assigned homework (delineated in the GitBook chapter for that week). This course follows a flipped classroom procedure, so you must complete the weekly homework to meaningfully participate in, and benefit from, the class meetings. Background knowledge We assume you have basic knowledge about multivariate statistics before entering this course. You do not need any prior experience working with R. If you wish to refresh your knowledge, we recommend the chapters on ANOVA, multiple regression, and exploratory factor analysis from Field’s Discovering Statistics using R. If you cannot access the Field book, many other introductory statistics textbooks cover these topics equally well. So, use whatever you have lying around from past statistics courses. You could also try one of the following open-access options: Applied Statistics with R Introduction to Modern Statistics Introduction to Statistical Learning "],["grading.html", "Grading", " Grading Your grade for the course is based on a “portfolio” composed of the three take-home assignments: Latent variable modeling Deadline: 2022-10-07 23:59 Group assignment Contributes 25% of your course grade Path modeling Deadline: 2022-10-28 23:59 Group assignment Contributes 25% of your course grade Full Structural equation modeling Deadline: 2022-11-11 23:59 Individual assignment Contributes 50% of your course grade The specifics of the assignments will soon be given as a seperate chapter in this GitBook. "],["attendance.html", "Attendance", " Attendance Attendance is not mandatory, but we strongly encourage you to attend all lectures and practicals. In our experience, students who actively participate tend to pass the course, whereas those who do not participate tend to drop out or fail. The lectures and practicals build on each other, so, in the unfortunate event that you have to miss a class meeting, please make sure you have caught up with the material before the next session. "],["software-setup.html", "Software Setup", " Software Setup This Chapter helps you prepare for the course. It shows how to install R and RStudio on your computer. We’ll also provide some general information on R and how you can get help if you get error messages. If you’re already using R, all of this might be nothing new for you. You may skip this chapter then. If you have never used R before, this Chapter is essential, as it gives you some input on how R works and how we can use it for our data analyses. "],["typeographic-conventions.html", "0.1 Typeographic Conventions", " 0.1 Typeographic Conventions Throughout this GitBook, we (try to) use a consistent set of typographic conventions: Functions are typeset in a code font, and the name of the function is always followed by parentheses E.g., sum(), mean() Other R objects (e.g., data objects, function arguments) are in also typeset in a code font but without parentheses E.g., seTE, method.tau Sometimes, we’ll use the package name followed by two colons (::, the so-called *scope-resolution operator), like lavaan::sem(). This command is valid R code and will run if you copy it into your R console. The lavaan:: part of the command tells R that we want to use the sem() from the lavaan package. "],["installing-software.html", "0.2 Installing software", " 0.2 Installing software Before we start the course, we have to install three things: R: A free program for statistical programming RStudio: An integrated development environment (IDE) which makes it easier to work with R. Several packages: Separate pieces of ‘add-on’ software for R with functions to do specific analyses. Packages also include documentation describing how to use their functions and sample data. 0.2.1 Installing R The latest version of R is available here. Click the appropriate link for your operating system and follow the instructions for installing the latest stable release. Depending on which OS you select, you may be given an option to install different components (e.g., base, contrib, Rtools). For this course, you will only need the base package. 0.2.2 Installing RStudio Download the Free Desktop version of RStudio from the download page of the RStudio website. 0.2.3 Installing packages To participate in this course, you will need a few essential R packages. Here’s an overview of the packages and why we need them: Package Description lavaan A sophisticated and user-friendly package for structural equation modeling ggplot2 A flexible and user-friendly package for making graphs tidySEM Plotting and tabulating the output of SEM-models semTools Comparing models, establishing measurement invariance across groups psych Descriptive statistics foreign Loading data from SPSS ‘.sav’ files readxl Loading data from Excel ‘.xslx’ files To install these packages, we use the install.packages() function in R. Open RStudio Inside RStudio, find the window named Console on left side of the screen. Copy the following code into the console and hit Enter/Return to run the command. install.packages(c(&quot;lavaan&quot;, &quot;ggplot2&quot;, &quot;tidySEM&quot;, &quot;semTools&quot;, &quot;psych&quot;, &quot;foreign&quot;, &quot;readxl&quot;), dependencies = TRUE) "],["getting-the-course-data.html", "0.3 Getting the course data", " 0.3 Getting the course data All of the data files you will need for the course are available in this SurfDrive directory. Follow the link to download a ZIP archive containing the data you will need to complete the practical exercises. Extract these data files to a convenient location on your computer. "],["r-tutorial-for-beginners-optional.html", "R tutorial for beginners (Optional)", " R tutorial for beginners (Optional) Welcome to the world of R! This tutorial is based on “R: How to get started” by Ihnwhi Heo, Duco Veen, and Rens van de Schoot. "],["who-r-you.html", "0.4 Who R you?", " 0.4 Who R you? R is a piece of free software for statistical computation and graphics. R is also fully open-source, which means anyone (even you!) can improve, develop, and contribute to R You can find the official manual from the R Core Team here: An introduction to R R itself looks a bit old-fashioned and tedious: "],["rstudio.html", "0.5 RStudio", " 0.5 RStudio Thankfully, we have a great user interface for R called RStudio! RStudio helps you use and learn R more easily. Although you are interacting with RStudio, you are still using R. Don’t ever write something like, “We conducted all analysis with RStudio…” For this course, all tutorials and practicals will use RStudio. 0.5.1 No ‘pane’, no gain! When you open RStudio, the screen should look something like the following image. You will notice that the window is divided into ‘panes’ (a pane is a division of a window). Before we explain these three panes, I want you to add another, which you will see if you open an R script. An R script is simply a plain-text file wherein you will write the R code that executes your analyses. When you open an R script (or create a new one, which is really the same thing), a fourth pane appears. 0.5.2 Create a new R script To create a new R script, Click the icon with a plus sign on the paper (highlighted below by the red square). When you click this icon, a new script is generated and appears in a fourth pane on the upper left side of the screen (if you’re using the default layout). Note that this script is not yet saved anywhere. If you close this script without saving, you will lose all its contents. The four panes help organize your workflow. Rstudio is an integrated development environment (IDE) that is meant to integrate all of the tasks you may need to do for any R-based analysis, programming, or development. Hence, in RStudio, the intended workflow will have you do everything in one window, and the four panes make this workflow more efficient (in theory, at least). 0.5.3 What do the four panes do? Note that the following descriptions apply to the default layout. You can change the orientation and content of the panes (although you must always have four). Source Pane: Located in the top left quadrant. This pane is also called the “Editor”, because this is where we edit scripts. We will usually type our code in the source pane. Console Pane: Located in the bottom left quadrant. This pane is for direct communication with R. We can type commands here that are immediately evaluated (whereas the commands in a script are only evaluated when we explicitly run them). Furthermore, all output from the R commands that we run (either via the console or a script) is printed in the console pane. The two panes on the right side of the window contain various tabs. Two of these tabs are especially useful. Environment Tab: Located in the upper right quadrant The environment tab contains all the ‘objects’ currently loaded in your R session. You can always check what objects are loaded under the environment tab. The ‘environment’ is also called the ‘workspace’ Plots Tab: Located in the lower right quadrant The plots tab shows any graphs and figures we draw via R commands. If you zoom by clicking the magnifying glass icon, you can see enlarged versions of the plots. "],["rstudio-projects.html", "0.6 RStudio projects", " 0.6 RStudio projects To keep all your work organized, you can use an RStudio Project. One advantage of using RStudio projects is that the project directory is automatically set as the working directory. If you save your data in the directory that contains the “.Rproj” file, you will can load the data without specifying the file path. 0.6.1 Starting a new project in Rstudio In Rstudio, click on the New project button: In the pop-up dialog, click New Directory. Click New Project. Type the desired directory name in the dialog box Give a meaningful name, e.g., “TCSM_Course”. Use the Browse button if you need to change the location of the directory that you will use to store this project. "],["loading-data.html", "0.7 Loading data", " 0.7 Loading data Statistical analysis cannot happen without data. In R, you can load data in various ways. Let’s go over a couple of these. To complete the following exercise, first download the LifeSat.sav data. 0.7.1 Via clicky-box options Click through the following menu options: File &gt; Import Dataset Choose the type of dataset. For this exercise, the data are stored as an SPSS .sav file. Thus, select the From SPSS option. You may encounter an Install Required Packages pop-up dialog with a message that asks you whether you want to install the haven package. A package is an a piece of add-on software for R. To do most analyses, you will need some packages above-and-beyond the Base R software. In this case, haven contains functions that allow R to read SPSS data files. Click ‘Yes’ to install the package. At this point, the Import Statistical Data pop-up dialog should appear. Provide the location of your data file in the File/URL field. You can type the file path directly. You can also click Browse to find the file via a GUI interface. You should now see your data in the Data Preview area. In the Import Options section, you can set the name and format of your data file. All of your your selections are being translated to the R-code required to load your file. In the Code Preview area, the steps required to load the data with your selected options are expressed in terms of code. Finally, click Import at the lower right side of the window to load your data. 0.7.2 Via R code Of course, we don’t have to use tedious clicky-box processes. Once you get more familiar with R, it will be much easier to load data with R code. For this exercise, we will use the read.spss() function from the foreign package. Copy the following code into an R-script. Note that this code assumes you have saved your data in the working directory (this is the project directory if you’re using an R project). Otherwise, you will need to specify the file path to your data file as the first argument. library(foreign) LifeSat &lt;- read.spss(&quot;LifeSat.sav&quot;, to.data.frame = TRUE, use.value.labels = FALSE) To run the code: Place your cursor on the first line of code. Press the Ctrl/Cmd + Enter keys together. The code should now be evaluated by R, and R will tell you the result of its actions in the console pane. 0.7.2.1 Data frames When we read data into R, the data are most often stored as a special type of object called a data frame. Data frames are the preferred way to handle data in R. Data frames can hold variables with different types (e.g., a numeric depression rating, a categorical grouping factor representing employment status, and a character string recording the response to an open-ended question). The option to.data.frame = TRUE in the read.spss() call above loads the data into a data frame. 0.7.3 From Excel files We can also load data that are stored in Excel files. One way to do so is via the readxl package. ## Load the package: library(readxl) ## Read the first sheet of &#39;LifeSat.xlsx&#39; into &#39;LifeSat&#39;: LifeSat &lt;- read_excel(&quot;LifeSat.xlsx&quot;, sheet = 1) "],["exploring-data-via-r-functions.html", "0.8 Exploring data via R functions", " 0.8 Exploring data via R functions If you look a bit more closely at the R code that we used to load the data, you will see two special commands: library() and read.spss(). These commands are called functions. Any R command that is written as a name followed by parentheses, e.g., mean(), is a function. Functions are the driving force behind all R-based data analysis. Functions tell R to perform a specific (potentially very complicated) task. Rather than having to write out along sequence of commands every time we want to do some task, we can simply call the appropriate function. When using functions, you need to provide appropriate inputs to specify the behavior of the function and give the function data on which to operate. These inputs are called function arguments. Let’s explore three new functions and their arguments. These functions can help us understand our data. 0.8.1 head() Our data comprise many rows. We can use the head() function to inspect the first several rows. To use the head() function, you only need to provide one argument: The name of the dataset. ## Inspect the first several rows head(LifeSat) “Wait, what is the hash tag (#) doing there?” Don’t be surprised. The hash tag creates a “comment”. I.e., a bit of text that will not be evaluated by R. Comments let us write notes to explain what a particular piece of code does. Comments are doubly useful. They can help others understand your code. They can also help you remember what the code does after some time away. Copy the preceding code into your R script and run it. If all went well, R should now show the first six rows of the dataset in the console. 0.8.2 str() We frequently want to know something about the types of variables in a dataset. For example, if you want to run an analysis of variance (ANOVA), the independent variable(s) should be categorical. In R, these variables would be represented by a special type of variable called a factor. Before running our analysis, we should check if are data satisfy this requirement. We can use the str() function to get some information about the structure of an R object (str is an abbreviation of structure). To run the str() function, you only need to provide one argument: the name of the dataset. ## Inspect the structure of the dataset str(LifeSat) ## &#39;data.frame&#39;: 98 obs. of 8 variables: ## $ LifSat : num 13 18 19 24 24 24 30 33 33 33 ... ## $ age : num 75 75 72 72 70 73 72 72 68 73 ... ## $ educ : num 6 5 5 6 5 6 6 5 7 6 ... ## $ gender : num 2 2 2 2 1 2 1 2 1 1 ... ## ..- attr(*, &quot;value.labels&quot;)= Named chr [1:2] &quot;2&quot; &quot;1&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;male&quot; &quot;female&quot; ## $ female : num 0 0 0 0 1 0 1 0 1 1 ... ## $ ChildSup: num 4 6 6 6 6 8 8 7 4 8 ... ## $ SpouSup : num 2 5 5 4 5 6 4 6 2 8 ... ## $ SES : num 3 1 1 1 1 1 1 2 2 2 ... ## ..- attr(*, &quot;value.labels&quot;)= Named chr [1:3] &quot;3&quot; &quot;2&quot; &quot;1&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;high SES&quot; &quot;middle SES&quot; &quot;low SES&quot; ## - attr(*, &quot;variable.labels&quot;)= Named chr [1:8] &quot;&quot; &quot;&quot; &quot;Years of education&quot; &quot;&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;LifSat&quot; &quot;age&quot; &quot;educ&quot; &quot;gender&quot; ... ## - attr(*, &quot;codepage&quot;)= int 1252 According to the output printed to the console, the LifeSat dataset is a data frame consisting of 98 observations of 8 variables. I.e., our dataset has 98 rows and 8 columns. We also get information about the contents of each column. For the fist column, we see $ LifSat : num 13 18 19 24 24 24 30 33 33 33 ... The LifSat variable is numeric (abbreviated as num). 13 is the value in the first row, 18 is the value in the second row, 19 is the value in the third row, and so on. 0.8.3 summary() We can get the descriptive statistics of the variables in a dataset with the summary() function. Again, we only need to provide one argument: the name of the dataset. summary(LifeSat) ## LifSat age educ gender ## Min. : 13.00 Min. :64.00 Min. : 4.000 Min. :1.000 ## 1st Qu.: 44.25 1st Qu.:68.25 1st Qu.: 6.000 1st Qu.:1.000 ## Median : 58.00 Median :70.00 Median : 6.000 Median :1.000 ## Mean : 57.86 Mean :70.26 Mean : 6.541 Mean :1.449 ## 3rd Qu.: 69.75 3rd Qu.:72.00 3rd Qu.: 7.000 3rd Qu.:2.000 ## Max. :100.00 Max. :75.00 Max. :12.000 Max. :2.000 ## female ChildSup SpouSup SES ## Min. :0.000 Min. : 3.000 Min. : 2.000 Min. :1.000 ## 1st Qu.:0.000 1st Qu.: 6.000 1st Qu.: 5.000 1st Qu.:1.000 ## Median :1.000 Median : 7.000 Median : 6.000 Median :2.000 ## Mean :0.551 Mean : 6.857 Mean : 6.061 Mean :1.918 ## 3rd Qu.:1.000 3rd Qu.: 8.000 3rd Qu.: 8.000 3rd Qu.:2.000 ## Max. :1.000 Max. :10.000 Max. :10.000 Max. :3.000 Run the above code and check the output in the console. You should see descriptive statistics for each variable in the dataset. E.g., for LifSat, the minimum value is 13, the median is 58, and the mean is 57.86. "],["plotting-data.html", "0.9 Plotting data", " 0.9 Plotting data It is almost always a good idea to visualize your data before you dive into a full statistical analysis. For example, you may like to know something about the nature of the relationship between two specific variables, the distribution of some set of values, etc. In this section, we will create three basic plots of our data. When we create these plots, the figures will appear in the plots tab in the lower right quadrant of the RStudio window. 0.9.1 R packages and ggplot2 We will use the ggplot2 package for plotting. This package is not part of the Base R installation. So, you must install ggplot2 yourself (you should have already done so in Section 0.2.3). You only need to install an R package once. R packages are just small software programs, so they must be installed just like any other piece of software. After installing, the package is accessible on your computer but not yet available for use in your current R session. You must first load the package. You need to load the package every time you want to use it in a new R session. After loading, the package contents (i.e., functions, help files, datasets) are initialized in memory and ready to use in your current R session. Loading a package is more-or-less equivalent to ‘opening’ a software program by clicking on its desktop icon. Run the following code to load the ggplot2 package. library(ggplot2) 0.9.2 Histogram We can use a histogram to visualize how the values of a continuous variable are distributed. ## Use the &#39;LifeSat&#39; data to create the plot ## Use the &#39;LifeSat$LifSat&#39; variable to define the X-axis ggplot(data = LifeSat, aes(x = LifSat)) + geom_histogram() # Create a histogram from the data/variable defined above 0.9.3 Boxplot A boxplot provides another useful visualization of a condituous variable’s distribution. We can also use boxplots to detect outliers. ## Same data/variable setup as above: ggplot(data = LifeSat, aes(x = LifSat)) + geom_boxplot() # Create a boxplot 0.9.4 Scatterplot A scatterplot provides a visual representation of the relationship between two variables. Since we are now plotting two variables, we need to define a y variable in addition to thex variable specified in the previous examples. ## Add the &#39;age&#39; variable on the Y-axis: ggplot(data = LifeSat, aes(x = LifSat, y = age)) + geom_point() # Create a scatterplot "],["manipulating-data.html", "0.10 Manipulating Data", " 0.10 Manipulating Data 0.10.1 Data types Recall the output of the str() function. One piece of information contained therein is the type of data stored in each column of our dataset. There a different abbreviations signifying different types of data. Abbreviation Type Description num Numeric All values are numbers (e.g., 1.02) chr Character All values are words log Logical Boolean flags: TRUE or FALSE factor Factor A special type of object with labels to represent the levels of a categorical variable 0.10.2 Factors Factors are a special type of data object that R uses to represent categorical variables. A factor is stored internally as a vector of integers where each group is represented by a different number. The groups also get descriptive labels. R knows that a factor is not numeric and will treat any factor as a nominal grouping variable for anlaysis. In the output from the str() function in 0.8.2, we see that the gender variable is stored as a numeric variable. You can confirm by running either of the following commands. is.numeric(LifeSat$gender) ## [1] TRUE class(LifeSat$gender) ## [1] &quot;numeric&quot; The gender varible is a binary grouping variable, so it should be stored as a factor. To convert gender to a factor, we can use the factor() function. ## Convert &#39;gender&#39; to a factor: LifeSat$gender &lt;- factor(LifeSat$gender) ## Check the results: is.numeric(LifeSat$gender) ## [1] FALSE class(LifeSat$gender) ## [1] &quot;factor&quot; str(LifeSat$gender) ## Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 2 2 2 2 1 2 1 2 1 1 ... We now see that gender has been converted to a factor with the levels “1” and “2”. We don’t have to settle for meaningless numeric labels, though. We can assign meaningful value labels by providing an appropriate input for the labels argument. ## Create a factor with meaningful labels: LifeSat$gender &lt;- factor(LifeSat$gender, labels = c(&quot;Female&quot;, &quot;Male&quot;)) ## Check the results: str(LifeSat$gender) ## Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 1 2 1 2 1 1 ... 0.10.3 Subsetting: Extracting one variable When working with data frames, we can extract a single variable (i.e., column) from the data using the dollar sign operator, $. As shown in the example below. # Extract the &#39;LifSat&#39; variable from &#39;LifeSat&#39;: LifeSat$LifSat ## [1] 13 18 19 24 24 24 30 33 33 33 33 33 35 35 37 37 41 41 41 ## [20] 43 43 43 44 44 44 45 47 48 48 48 50 51 51 52 53 53 53 53 ## [39] 54 55 55 56 56 56 57 58 58 58 58 58 58 59 59 60 61 61 63 ## [58] 63 63 65 66 67 67 67 67 68 68 68 69 69 69 69 69 70 70 70 ## [77] 71 72 74 74 76 77 77 78 78 79 79 79 81 81 82 83 85 86 87 ## [96] 91 99 100 In the above code, we ask R to extract the column named LifSat from the data frame named LifeSat. The result will be returned as another special type of object: a vector. 0.10.4 Subsetting: Extracting rows and columns We can also extract rectangular subsets of a data frame using the following convention: my_data_frame[row_numbers, column_numbers]. ## Extract the first four rows of the first two columns: LifeSat[1:4, 1:2] By leaving either rows or columns empty, we get all rows or columns: ## Extract all rows of first two columns: LifeSat[ , 1:2] ## Extract all columns of first two rows: LifeSat[1:2, ] We can refer to the columns by name, too: LifeSat[1:2, c(&quot;age&quot;, &quot;educ&quot;)] 0.10.5 Subsetting based on logical conditions We can also select rows or columns that satisfy logical conditions. In the following code, we select only the rows for which LifeSat$age is greater than 70. LifeSat[LifeSat$age &gt; 70, ] This approach works for any valid logical expression that will flag rows (or columns). Below, we select only the males and save the subset as a new object, LifeSat_male. LifeSat_male &lt;- LifeSat[LifeSat$gender == &quot;Male&quot;, ] str(LifeSat_male) ## &#39;data.frame&#39;: 44 obs. of 8 variables: ## $ LifSat : num 13 18 19 24 24 33 33 35 35 37 ... ## $ age : num 75 75 72 72 73 72 72 68 71 68 ... ## $ educ : num 6 5 5 6 6 5 12 6 4 7 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ female : num 0 0 0 0 0 0 0 0 0 0 ... ## $ ChildSup: num 4 6 6 6 8 7 5 6 7 4 ... ## $ SpouSup : num 2 5 5 4 6 6 6 6 7 4 ... ## $ SES : num 3 1 1 1 1 2 2 3 1 2 ... ## - attr(*, &quot;variable.labels&quot;)= Named chr [1:8] &quot;&quot; &quot;&quot; &quot;Years of education&quot; &quot;&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;LifSat&quot; &quot;age&quot; &quot;educ&quot; &quot;gender&quot; ... ## - attr(*, &quot;codepage&quot;)= int 1252 0.10.6 Changing cell values We can easily overwrite values in a dataset using the same type of subsetting operations shown above in combination with the assignment operator, &lt;-. The following code will extract the 5th entry in the LifSat variable. LifeSat[5, &quot;LifSat&quot;] ## [1] 24 Actually, it’s not really accurate to say that the above code “extracts” any value. The above command creates a new temporary object containing only the relevant value and prints the contents of that object to the console. All the subsetting examples above (other than the selection of all males) have done something similar. Rather than thinking about the [] or $ selection operators as ways of extracting pieces of a data object, it’s more appropriate to think about these operators as selecting, highlight, activating, nominating (or some other such concept) the referenced elements. Once the elements are so selected, we can also overwrite their original values. We only need to assign new values to the subset. To demonstrate, let’s overwrite the value selected above with 10. ## Overwrite the 5th &#39;LifSat&#39; value: LifeSat[5, &quot;LifSat&quot;] &lt;- 10 ## Check the result: LifeSat[5, &quot;LifSat&quot;] ## [1] 10 "],["getting-help.html", "0.11 Getting Help", " 0.11 Getting Help As you start to apply the techniques described in this guide, you will soon have questions that the guide does not answer. This section describes a few tips on how to get help answering these questions. Every function in R has documentation (i.e., a help file). To see this file in RStudio, select the name of the function in your script, and press F1, or run the command ? followed by the name of the function (e.g., ?aov). The second option works outside of RStudio, too. If you get stuck, start with Google. Typically, adding “R” to a search is enough to return relevant results (e.g., “exploratory factor analysis R”). Google is particularly useful for error messages. If you get an error message that you don’t understand, try googling it. Someone else has almost certainly been confused by the same message in the past, and there will be help somewhere on the web. If the error message isn’t in English, run Sys.setenv(LANGUAGE = \"en\") and re-run the code that produced the error (you’re more likely to find help for English error messages.) If Google doesn’t help, try stackoverflow. Before posting your question, spend some time searching the site for an existing answer (active contributors really hate it when you ask a question that has already been answered on the site). Including R restricts your search to questions and answers that use R. Lastly, if you find errors (or typos!) in this guide’s text or R syntax, feel free to contact me (Kyle Lang). "],["introduction.html", "1 Introduction", " 1 Introduction Homework before lecture Work your way through the first two chapters of the GitBook online: https://cjvanlissa.github.io/TCSM/index.html. This will help you install the software required for the course, and get the data into R. You can skip the optional sections if you are already familiar with R. Read the article by Smaldino (skip the red sections; the yellow section is optional). Answer the reading questions (on Blackboard). Plan a call with your learning team to discuss the second question. Reference: Smaldino, P. E. (2017). Models are stupid, and we need more of them. Computational social psychology, 311-331. Lecture content We start with a brief introduction to the course, its goals and rules and the idea of statistical modelling. In this lecture we will introduce the type of models that will come across in this course. We will shortly discuss the concepts of model simplicity/complexity, model fit, the graphical display and the interpretation of different model parameters. Homework before practical Perform the take-home exercise Regression (Chapter 3, Week 1 Home) before coming to the practical. Practical content During the practical you will work on the class exercise about regression and SEM models. "],["lecture.html", "1.1 Lecture", " 1.1 Lecture Download slides The first lecture will be used to introduce the concept of fitting models to data and explain some important concepts and notation that will be used during this course. "],["reading-questions-1.html", "1.2 Reading questions", " 1.2 Reading questions Reference Smaldino, P. E. (2017). Models are stupid, and we need more of them. Computational social psychology, 311-331. SKIP PAGES 322 - 327 Questions What are the differences between a “verbal model” and a “formal model”? As explained in the paragraph “A Brief Note on Statistical Models”, formal models are not the same as statistical models. Still, we can learn a lot from Smaldino’s approach. Write down three insights from this paper that you would like to apply to your statistical modelling during this course, and discuss with your learning group. "],["at-home-exercises.html", "1.3 At-Home Exercises", " 1.3 At-Home Exercises Open the data file LifeSat.sav. library(foreign) data &lt;- read.spss(&quot;LifeSat.sav&quot;, to.data.frame = TRUE) 1.3.1 Make a table with descriptive statistics for the variables: LifSat, educ, ChildSup, SpouSup, and age. What is the average age in the sample? And the range (youngest and oldest child)? Hint: Use library(tidySEM); descriptives(); [] Click for explanation The package tidySEM contains a function to describe data. Install and load the package, then use the descriptives() function. Alternatively, you can also use the describe() function in the psych package. library(tidySEM) descriptives(data[, c(&quot;LifSat&quot;, &quot;educ&quot;, &quot;ChildSup&quot;, &quot;SpouSup&quot;, &quot;age&quot;)]) &lt;&gt; 1.3.2 Perform a simple regression with LifSat as the dependent variable and educ as the independent variable. Hint: The function lm() (short for linear model) conducts linear regression. The functions summary() provides relevant summary statistics for the model. It can be helpful to store the results of your analysis in an object, too. Click for explanation results &lt;- lm(LifSat ~ educ, data) summary(results) ## ## Call: ## lm(formula = LifSat ~ educ, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -43.781 -11.866 2.018 12.418 43.018 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 35.184 7.874 4.469 2.15e-05 *** ## educ 3.466 1.173 2.956 0.00392 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.64 on 96 degrees of freedom ## Multiple R-squared: 0.08344, Adjusted R-squared: 0.0739 ## F-statistic: 8.74 on 1 and 96 DF, p-value: 0.003918 &lt;&gt; 1.3.3 Do the same with age as the independent variable. Click for explanation results &lt;- lm(LifSat ~ age, data) summary(results) ## ## Call: ## lm(formula = LifSat ~ age, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -35.321 -14.184 3.192 13.593 40.626 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 200.2302 52.1385 3.840 0.00022 *** ## age -2.0265 0.7417 -2.732 0.00749 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.75 on 96 degrees of freedom ## Multiple R-squared: 0.07215, Adjusted R-squared: 0.06249 ## F-statistic: 7.465 on 1 and 96 DF, p-value: 0.007487 &lt;&gt; 1.3.4 Again with ChildSup as the independent variable. Click for explanation results &lt;- lm(LifSat ~ ChildSup, data) summary(results) ## ## Call: ## lm(formula = LifSat ~ ChildSup, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.32 -12.14 0.66 12.41 44.68 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.559 8.342 4.502 1.89e-05 *** ## ChildSup 2.960 1.188 2.492 0.0144 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.86 on 96 degrees of freedom ## Multiple R-squared: 0.06076, Adjusted R-squared: 0.05098 ## F-statistic: 6.211 on 1 and 96 DF, p-value: 0.01441 &lt;&gt; 1.3.5 Perform a multiple regression with LifSat as the dependent variable and educ, age and ChildSup as the independent variables. Hint: You can use the + sign to add multiple variables to a model. Click for explanation results &lt;- lm(LifSat ~ educ + age + ChildSup, data) summary(results) ## ## Call: ## lm(formula = LifSat ~ educ + age + ChildSup, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.98 -12.56 2.68 11.03 41.91 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 134.9801 53.2798 2.533 0.0130 * ## educ 2.8171 1.1436 2.463 0.0156 * ## age -1.5952 0.7188 -2.219 0.0289 * ## ChildSup 2.4092 1.1361 2.121 0.0366 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.92 on 94 degrees of freedom ## Multiple R-squared: 0.1741, Adjusted R-squared: 0.1477 ## F-statistic: 6.603 on 3 and 94 DF, p-value: 0.0004254 &lt;&gt; 1.3.6 Compare the results under 1.e with those obtained under 1.b-1.d. What do you notice when you compare the regression parameter for each of the three predictors in the multiple regression with the corresponding regression parameters obtained in the simple regressions? "],["in-class-exercises.html", "1.4 In-Class Exercises", " 1.4 In-Class Exercises During the practical you will work on some exercises about ANOVA and ANCOVA using regression and path modeling. Note that ANOVA and ANCOVA are special cases of regression, as discussed during MTS3 or a similar course. How to perform an ANOVA/ANCOVA as a regression analysis is prerequisite knowledge. This practical we will work on these topics (ANOVA, ANCOVA, regression and how they are related). If you need to refresh your knowledge you could use the internet to find information or you could look it up in a book on statistics, for example @field2012discovering (The chapters on ANOVA, Factorial ANOVA, and ANCOVA (11.6)). We start with two exercises in which you have to explore your data and perform a regression analysis, ANOVA and an ANCOVA. You will also practice with performing an ANCOVA as a regression analysis in exercise 3 today. 1.4.1 Loading data Open the file Sesam.sav: # Library for reading SPSS files: library(foreign) # Load the data and put them in the object called &quot;data&quot; data &lt;- read.spss(&quot;sesam.sav&quot;, to.data.frame = TRUE, use.value.labels = FALSE) This file is part of a larger dataset that evaluates the impact of the first year of the Sesame Street television series. Sesame Street is mainly concerned with teaching preschool related skills to children in the 3-5 year age range. The following variables will be used in this exercise: age measured in months prelet knowledge of letters before watching Sesame Street (range 0-58) prenumb knowledge of numbers before watching Sesame Street (range 0-54) prerelat knowledge of relations before watching Sesame Street (range 0-17) peabody vocabulary maturity before watching Sesame Street (range 20-120) postnumb knowledge of numbers after a year of Sesame Street (range 0-54) 1.4.2 1.4.2.1 What is the level of measurement of each of the variables? Click for explanation In the ‘Environment’ panel in the top right corner of the screen, click the arrow in the next to the object called ‘data’. Alternatively, run the rode: head(data). 1.4.2.2 What is the average age in the sample? And the range (youngest and oldest child)? Hint: Use library(tidySEM); descriptives() Click for explanation As in the take home exercises, use the function descriptives() from the tidySEM package to describe the data: library(tidySEM) descriptives(data) 1.4.2.3 What is the average gain in knowledge of numbers? Provide both the mean and the standard deviation. Hint: Use the &lt;- operator to assign to a new variable in data. You can use descriptives(), or the functions mean() and sd(). Click for explanation Create a new variable that represents the difference between pre- and post-test scores: data$dif &lt;- data$postnumb - data$prenumb There are specialized functions to obtain the mean and sd: mean(data$dif) ## [1] 9.158333 sd(data$dif) ## [1] 9.682401 1.4.2.4 Choose an appropriate graph to present the gain scores. What did you choose and why? Hint: As explained in the introductory chapters, you can use ggplot and add a histogram, density plot, or boxplot: geom_histogram(); geom_density(); geom_boxplot() Click for explanation library(ggplot2) p &lt;- ggplot(data, aes(x = dif)) p + geom_histogram() p + geom_density() p + geom_boxplot() 1.4.2.5 Can you think of a graph based on two variables that is informative? What is it and how is it informative? Hint: A useful plotting function for a bivariate distribution is the scatterplot: geom_point() Click for explanation #Possible variables would be the pre- and post measurement ggplot(data, aes(x = prenumb, y = postnumb)) + geom_point() 1.4.2.6 Which of the variables age, prelet, prenumb, prerelat and peabody are related to postnumb? Use Pearson’s correlations (cor()). You don’t need to check assumptions. If you want p-values for the correlations, use the function corr.test() from the psych package instead. Hint: The function corr.test() from the psych package provides Pearson’s correlationsand p-values (the base R function cor() does not provide p-values). Select variables by name from a data.frame object (like data) using the following syntax: data[, c(\"each\", \"variable\", \"name\")]. Click for explanation library(psych) corr.test(data[, c(&quot;age&quot;, &quot;prelet&quot;, &quot;prenumb&quot;, &quot;prerelat&quot;, &quot;peabody&quot;, &quot;postnumb&quot;)]) ## Call:corr.test(x = data[, c(&quot;age&quot;, &quot;prelet&quot;, &quot;prenumb&quot;, &quot;prerelat&quot;, ## &quot;peabody&quot;, &quot;postnumb&quot;)]) ## Correlation matrix ## age prelet prenumb prerelat peabody postnumb ## age 1.00 0.33 0.43 0.44 0.29 0.34 ## prelet 0.33 1.00 0.72 0.47 0.40 0.50 ## prenumb 0.43 0.72 1.00 0.72 0.61 0.68 ## prerelat 0.44 0.47 0.72 1.00 0.56 0.54 ## peabody 0.29 0.40 0.61 0.56 1.00 0.52 ## postnumb 0.34 0.50 0.68 0.54 0.52 1.00 ## Sample Size ## [1] 240 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## age prelet prenumb prerelat peabody postnumb ## age 0 0 0 0 0 0 ## prelet 0 0 0 0 0 0 ## prenumb 0 0 0 0 0 0 ## prerelat 0 0 0 0 0 0 ## peabody 0 0 0 0 0 0 ## postnumb 0 0 0 0 0 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option The use of data[,] follows the conventions of matrix indexation: You can select rows (the horizontal lines) like this, data[i, ], and columns (the vertical lines) like this, data[ ,j], where i are the rows and j are the columns you want to select. As you can see in the example, you can select multiple columns using c( … , … ). 1.4.2.7 Can age and prenumb be used to predict postnumb? If so, discuss the substantial importance of the model and the significance and substantial importance of the separate predictors. Hint: The function lm() (short for linear model) conducts linear regression. The functions summary() provides relevant summary statistics for the model. It can be helpful to store the results of your analysis in an object, too. Click for explanation results &lt;- lm(formula = postnumb ~ age + prenumb, data = data) summary(results) ## ## Call: ## lm(formula = postnumb ~ age + prenumb, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.130 -6.456 -0.456 5.435 22.568 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.4242 5.1854 1.432 0.154 ## age 0.1225 0.1084 1.131 0.259 ## prenumb 0.7809 0.0637 12.259 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.486 on 237 degrees of freedom ## Multiple R-squared: 0.4592, Adjusted R-squared: 0.4547 ## F-statistic: 100.6 on 2 and 237 DF, p-value: &lt; 2.2e-16 1.4.2.8 Provide the null hypotheses and the alternative hypotheses of the model in 1.g. Click for explanation The null-hypotheses of the model pertain to the variance explained: \\(\\rho^2\\) (that’s Greek letter rho, for the population value of \\(\\rho^2\\)). \\(H_0: \\rho^2 = 0\\) \\(H_a: \\rho^2 &gt; 0\\) 1.4.2.9 Consider the path model below. How many regression coefficients are estimated in this model? And how many variances? And how many covariances? How many degrees of freedom does this model have? (\\(df = N_{obs} – N_{par}\\), see slides Lecture 1). 1.4.2.10 Consider a multiple regression analysis with three continuous independent variables, tests in language, history and logic, and one continuous dependent variable, a score on a math test. We want to know whether the various tests can predict the math score. Sketch a path model for this analysis (there are examples in the lecture slides of week 1). How many regression parameters are there? How many variances could you estimate? How many covariances could you estimate? How many degrees of freedom does this model have? 1.4.3 Open the file Drivers.sav: # Load the data and put them in the object called &quot;data&quot; data &lt;- read.spss(&quot;Drivers.sav&quot;, to.data.frame = TRUE) 1.4.3.1 Research question 1 (ANOVA): Does talking on the phone interfere with people’s driving skills? The IV for this reseach question is condition, with conditions: hand-held phone hands-free phone control The DV is reaction time in milliseconds in a driver simulation test, in variable RT. 1.4.3.2 Perform the ANOVA. You can use lm(y ~ -1 + x) to remove the intercept from a regression with dummies, and get a separate mean for each group. The function aov() is an alternate interface for lm() that reports results in a way that matches the conventions for ANOVA analyses more closely. Click for explanation You can use summary(lm(y ~ -1 + x)) to get the means for each group: results &lt;- lm(formula = RT ~ -1 + condition, data = data) summary(results) ## ## Call: ## lm(formula = RT ~ -1 + condition, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -317.50 -71.25 2.97 89.55 243.45 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## conditionhand-held 654.50 29.08 22.51 &lt;2e-16 *** ## conditionhands-free 617.55 29.08 21.24 &lt;2e-16 *** ## conditioncontrol 553.75 29.08 19.04 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 130.1 on 57 degrees of freedom ## Multiple R-squared: 0.9586, Adjusted R-squared: 0.9564 ## F-statistic: 440 on 3 and 57 DF, p-value: &lt; 2.2e-16 And you can use aov() to get the sum of squares for the factor: results &lt;- aov(formula = RT ~ condition, data = data) summary(results) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## condition 2 103909 51954 3.072 0.0541 . ## Residuals 57 964082 16914 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 1.4.3.3 What are the assumptions you need to check? Click for explanation We can check several assumptions: Presence of outliers Normality of residuals Homogeneity of residuals Let’s deal with them in order. 1.4.3.3.1 Presence of outliers: In Y-space We can check the range of the standardized (scale()) residuals for outliers in Y-space. The residuals are inside of the results object, so we can just extract them, standardize them, and get the range: range(scale(results$residuals)) ## [1] -2.483778 1.904491 What is your conclusiong about the outliers? 1.4.3.3.2 Normality of residuals We can check the normality of residuals using a QQplot. qqnorm(results$residuals) qqline(results$residuals) There appears to be some mild deviation from normality at the extremes. You can also test for normality with the shapiro.test(x) function: shapiro.test(results$residuals) ## ## Shapiro-Wilk normality test ## ## data: results$residuals ## W = 0.98367, p-value = 0.6013 1.4.3.3.3 Homogeneity of Variances The bartlett.test() function provides a parametric K-sample test of the equality of variances. This test has the same hypotheses as the Levene’s test. bartlett.test(formula = RT~condition, data = data) ## ## Bartlett test of homogeneity of variances ## ## data: RT by condition ## Bartlett&#39;s K-squared = 2.7203, df = 2, p-value = 0.2566 It can also be nice to use a paneled boxplot to visualize the distributions. For this, we will use ggplot2. This time, we introduce a new command, theme_bw(): A theme for the plot that conforms to APA standards. We can apply this theme to any figure created using ggplot(): library(ggplot2) ggplot(data, aes(y = RT, group = condition)) + geom_boxplot() + theme_bw() 1.4.3.4 Explain for each of the assumptions why they are important to check. 1.4.3.5 What are your conclusions regarding the assumption checks? Click for explanation There are no outliers in X-space, no evidence for (severe) deviations from normality of residuals, and no evidence for (severe) heteroscedasticity. 1.4.3.6 Answer the research question. Hint: Use summary() and TukeyHSD(). Click for explanation We can examine the overall F-test, which is significant: summary(results) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## condition 2 103909 51954 3.072 0.0541 . ## Residuals 57 964082 16914 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TukeyHSD(results) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = RT ~ condition, data = data) ## ## $condition ## diff lwr upr p adj ## hands-free-hand-held -36.95 -135.917 62.017041 0.6434900 ## control-hand-held -100.75 -199.717 -1.782959 0.0451401 ## control-hands-free -63.80 -162.767 35.167041 0.2750829 Post-hoc tests with Bonferroni correction can be obtained using TukeyHSD(results). We notice that none of these comparisons are significant. However, the research question was Does talking on the phone interfere with peoples driving skills? There are two conditions for talking on the phone. We could thus test a planned contrast of these two conditions against the control condition, instead of all possible post-hoc tests: The standard contrasts are dummy coded: contrasts(data$condition) ## hands-free control ## hand-held 0 0 ## hands-free 1 0 ## control 0 1 We can replace these with planned contrasts for “phone” vs control, and hand-held vs hands-free: contrasts(data$condition) &lt;- cbind(phoneVcontrol = c(-1, -1, 2), handVfree = c(-1, 1, 0)) results &lt;- aov(RT ~ condition, data) # Ask for the lm summary, which gives you t-tests for the planned contrasts: summary.lm(results) ## ## Call: ## aov(formula = RT ~ condition, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -317.50 -71.25 2.98 89.55 243.45 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 608.60 16.79 36.248 &lt;2e-16 *** ## conditionphoneVcontrol -27.42 11.87 -2.310 0.0245 * ## conditionhandVfree -18.47 20.56 -0.898 0.3727 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 130.1 on 57 degrees of freedom ## Multiple R-squared: 0.09729, Adjusted R-squared: 0.06562 ## F-statistic: 3.072 on 2 and 57 DF, p-value: 0.05408 1.4.3.7 Research question 2 (ANCOVA): Are there differences in reaction time between the conditions when controlling for age? 1.4.3.8 What are the assumptions you need to check? Click for explanation Assumptions for ANCOVA are the same as for ANOVA (no outliers, normality of residuals, homoscedasticity). ANCOVA has the following additional assumptions: Homogeneity of regression slopes for the covariate (no interaction between factor variable and covariate) The covariate is independent of the treatment effects. I.e. there is no difference in the covariate between the groups of the independent variable. 1.4.3.9 Explain for each of the assumptions why they are important to check. 1.4.3.10 Check the assumptions of ANCOVA. Hint: Within formulas, you can use * instead of + to include interaction effects. Click for explanation 1.4.3.10.1 Homogeneity of regression slopes Add the interaction to the model and test whether the interaction is significant: results_age &lt;- aov(RT ~ condition + age, data) results_age_int &lt;- aov(RT ~ condition * age, data) summary(results_age_int) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## condition 2 103909 51954 4.532 0.0151 * ## age 1 320454 320454 27.955 2.3e-06 *** ## condition:age 2 24622 12311 1.074 0.3488 ## Residuals 54 619005 11463 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #Or you could use `anova()` to compare two different models anova(results_age, results_age_int) What would your conclusion be about this assumption? Click for explanation The interaction is NOT significant; no evidence for violation of the assumption. 1.4.3.10.2 The covariate is independent of the treatment effects results_indep &lt;- aov(age ~ condition, data) summary(results_indep) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## condition 2 137 68.55 0.659 0.521 ## Residuals 57 5926 103.97 What would your conclusion be about this assumption? Click for explanation The covariate is not significantly related to treatment effect. The assumption is met. 1.4.3.11 Answer the research question. (Do you have to include the interaction or not?) Click for explanation results &lt;- aov(formula = RT ~ condition + age, data = data) TukeyHSD(results) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = RT ~ condition + age, data = data) ## ## $condition ## diff lwr upr p adj ## hands-free-hand-held -36.95 -118.5708 44.67082 0.5242511 ## control-hand-held -100.75 -182.3708 -19.12918 0.0119407 ## control-hands-free -63.80 -145.4208 17.82082 0.1533777 The handheld-condition has a significant higher reaction time than the control condition 1.4.4 Open the file Sesam2.sav. # Load the data and put them in the object called &quot;data&quot; data &lt;- read.spss(&quot;Sesam2.sav&quot;, to.data.frame = TRUE) Use postnumb as the dependent variable in all the following analyses. 1.4.4.1 Viewcat is a factor variable, but is not coded as such in the data. Turn it into a factor. Afterwards, make sure that viewcat=1 is the reference group in the contrasts, i.e., the group that is identified by zero scores on all the associated dummy variables. Hint: Use &lt;- factor() and contrasts(). Click for explanation data$VIEWCAT &lt;- factor(data$VIEWCAT) contrasts(data$VIEWCAT) ## 2 3 4 ## 1 0 0 0 ## 2 1 0 0 ## 3 0 1 0 ## 4 0 0 1 1.4.4.2 Perform a multiple regression analysis with just the viewcat dummies as predictors. Click for explanation results &lt;- lm(POSTNUMB ~ VIEWCAT, data) summary(results) ## ## Call: ## lm(formula = POSTNUMB ~ VIEWCAT, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.474 -7.942 0.240 8.526 25.240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.760 2.316 8.102 8.95e-14 *** ## VIEWCAT2 9.331 2.900 3.218 0.00154 ** ## VIEWCAT3 14.714 2.777 5.298 3.49e-07 *** ## VIEWCAT4 18.032 2.809 6.419 1.24e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.58 on 175 degrees of freedom ## Multiple R-squared: 0.2102, Adjusted R-squared: 0.1967 ## F-statistic: 15.53 on 3 and 175 DF, p-value: 5.337e-09 1.4.4.3 What do the regression coefficients represent? How can you determine the average postnumb score for each of the viewcat categories, based on the regression parameters? 1.4.4.4 Make a coloured scatter plot with age on the x-axis and postnumb on the y-axis. Colour the dots according to the their viewcat category. How do you interpret the differences in slopes of these four fit lines? Hint: Use ggplot() and geom_point(); use the argument aes(colour = ‘…’) to map colour to a certain variable. A new command is geom_smooth() : This plots a smooth line (like a regression line). Click for explanation We will use ggplot again: ggplot(data, aes(x = AGE, y = POSTNUMB, colour = VIEWCAT)) + geom_point() + # For scatterplot geom_smooth(method = &quot;lm&quot;, se = FALSE) + # For regression lines theme_bw() # For a pretty theme 1.4.4.5 Add an interaction between age and viewcat to the regression analysis. Hint: An interaction is created by multiplying two variables. You can multiply with * in the formula of lm(). Click for explanation results_interaction &lt;- lm(POSTNUMB ~ VIEWCAT*AGE, data) summary(results_interaction) ## ## Call: ## lm(formula = POSTNUMB ~ VIEWCAT * AGE, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.8371 -8.2387 0.6158 8.7988 22.5611 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -18.7211 15.5883 -1.201 0.2314 ## VIEWCAT2 9.9741 20.6227 0.484 0.6293 ## VIEWCAT3 23.5825 19.3591 1.218 0.2248 ## VIEWCAT4 34.3969 19.3600 1.777 0.0774 . ## AGE 0.7466 0.3074 2.429 0.0162 * ## VIEWCAT2:AGE -0.0175 0.4060 -0.043 0.9657 ## VIEWCAT3:AGE -0.1930 0.3782 -0.510 0.6104 ## VIEWCAT4:AGE -0.3416 0.3770 -0.906 0.3663 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.99 on 171 degrees of freedom ## Multiple R-squared: 0.3046, Adjusted R-squared: 0.2762 ## F-statistic: 10.7 on 7 and 171 DF, p-value: 3.79e-11 1.4.4.6 Perform a sequential multiple regression. Include age and viewcat as the predictors in the first analysis. Add the interaction term in the second analysis. Make sure to obtain information about the change in R-square! Hint: Use anova() to compare two regression models. Click for explanation results_main &lt;- lm(POSTNUMB ~ VIEWCAT + AGE, data) anova(results_main, results_interaction) 1.4.4.7 Sketch path models of both steps of the regression analysis (on paper). Click for explanation Step 1: Step 2: 1.4.5 Question 3.h Write down the regression equations of both steps of the sequential analysis. Click for explanation \\(Postnumb_i = b_0 + b_1D_{view2i} + b_2D_{view3i} + b_3D_{view4i} + b_4Age_i + \\epsilon_i\\) \\[ \\begin{aligned} Postnumb_i = b_0 + &amp;b_1D_{view2i} + b_2D_{view3i} + b_3D_{view4i} + b_4Age_i +\\\\ &amp;b_5D_{view2i}Age_i + b_6D_{view3i}Age_i + b_7D_{view4i}Age_i + \\epsilon_i \\end{aligned} \\] 1.4.6 Question 3.i Write down the null hypothesis that is tested to determine whether there is an interaction between age and viewcat. Click for explanation \\(H_0: b_5 = b_6 = b_7 = 0\\) 1.4.6.1 Indicate for each parameter in the second regression model what it means. Also write down the regression equation for each of the four categories of viewcat separately. Click for explanation Parameter Meaning b_0 Intercept; the predicted value of postnumb for someone of age 0 in viewcat 1 b_1 Slope of the dummy for viewcat 2; difference in the predicted value of postnumb for someone aged 0 in category 2, compared to category 1 b_4 The effect of age for someone in viewcat 1 b_5 Difference in the effect of age for someone in viewcat 2, compared to viewcat 1 b_7 Difference in the effect of age for someone in viewcat 4, compared to viewcat 1 For viewcat 1: \\(Postnumb_i = b_0 + b_4Age_i + \\epsilon_i\\) For viewcat 2: \\(Postnumb_i = b_0 + b_4Age_i + b_1D_{view2i} + b_4Age_i + b_5D_{view2i}Age_i + \\epsilon_i\\) Etc. 1.4.6.2 What do you conclude about the interaction between age and viewcat? 1.4.6.3 Note that you can also look at this problem as an ANCOVA. What are the research question and null hypothesis in this case? Click for explanation RQ: Is there a significant difference between the marginal means of postnumb by viewcat, after controlling for age? \\(H_0:\\) After controling for age, the mans of postnumb are equal in all groups. 1.4.6.4 Perform this analysis as an ANCOVA. Hint: Add -1 to a formula to drop the intercept. Click for explanation To drop the intercept from the analysis, and estimate the marginal means for all viewcat categories, we can add -1 (minus the intercept) to the formula: results_ancov &lt;- aov(POSTNUMB~AGE+VIEWCAT-1, data) Examine the parameter estimates of the ANCOVA. What do the parameter estimates represent? Click for explanation We use summary.lm() again to obtain the parameter estimates: summary.lm(results_ancov) ## ## Call: ## aov(formula = POSTNUMB ~ AGE + VIEWCAT - 1, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.680 -8.003 -0.070 8.464 22.635 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## AGE 0.5750 0.1221 4.708 5.08e-06 *** ## VIEWCAT1 -10.1056 6.5091 -1.553 0.122 ## VIEWCAT2 -0.9603 6.3865 -0.150 0.881 ## VIEWCAT3 3.7546 6.4760 0.580 0.563 ## VIEWCAT4 6.8159 6.5414 1.042 0.299 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.94 on 174 degrees of freedom ## Multiple R-squared: 0.8973, Adjusted R-squared: 0.8943 ## F-statistic: 304 on 5 and 174 DF, p-value: &lt; 2.2e-16 The parameter estimates are the means of each VIEWCAT category when age = 0. "],["formative-assessment.html", "1.5 Formative Assessment", " 1.5 Formative Assessment A formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material. Complete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention. Question 1: In the equation \\(Y_i = a + bX_i + e_i\\), what are the ‘model parameters’? \\(Y_i, X_i\\) and \\(e_i\\) \\(a\\) and \\(b\\) \\(Y\\) and \\(X\\) \\(a\\) and \\(bX_i\\) Question 2: Multiple regression and ANCOVA are statistically equivalent. FALSE TRUE Question 3: The error term (\\(\\epsilon_i\\)) in regression equations reflects how much the observed scores of individuals differ from their predicted scores. FALSE TRUE Question 4: A model with more degrees of freedom is… more complex more simple Question 5: What kind of model is depicted above? A measurement model ANOVA with dummies Multiple regression A path model Question 6: What are degrees of freedom? The number of unique pieces of information minus number of parameters Number of participants minus number of parameters The number of unique pieces of information The number of parameters Question 7: A psychologist administers a test intended to measure intelligence. Participants complete different puzzles and answer different questions. From a measurement theory point of view, what kind of variable is intelligence in this context? A measurement variable An observed variable A dependent variable A latent variable Question 8: A model with more degrees of freedom will… fit the data better fit the data worse Question 9: In OLS regression, a model is fit to the individual participant data. By contrast, regression in structural equation modeling fits a model to the observed covariance matrix. FALSE TRUE "],["efa.html", "2 EFA", " 2 EFA Homework before the lecture Read this week’s literature and finish the reading questions before coming to the lecture (see Blackboard for direct URLs) Make sure to also answer the reading questions (on Blackboard). Preacher, Kristopher J. and MacCullum, Robert C. (2003) Repairing Tom Swift’s Electric Factor Analysis Machine, Understanding Statistics 2(1) 13-43. Kestilä, Elina (2006) Is There Demand for Radical Right Populism in the Finnish Electorate? Scandinavian Political Studies 29(3),169-191 Lecture content This lecture forms an introduction to latent variables and scaling procedures. Different aspects of exploratory factor analysis will be discussed. Most notably we focus on the differences between Principal Component Analyses (PCA) and Factor Analysis, estimation and extraction methods, and factor axes rotations. The researcher has to take decisions on all these aspects when (s)he wants to analyse whether questions form a scale or not. Furthermore, reliability and factor scores are discussed as methods to construct and evaluate the properties of a scale. Homework before the practical Perform the take home exercise Exploratory Factor Analysis (EFA) before coming to the practical. Practical content During the practical, the take home exercise EFA is shortly discussed. After this, students can work on the class exercise EFA. "],["lecture-1.html", "2.1 Lecture", " 2.1 Lecture Download slides How do I test whether I measure what I intend to measure? In the social sciences we often try to measure things indirectly. For example, someone’s IQ or socio-economic status cannot be measured with just one question; we need more questions that each measure different aspects of these constructs. The method that is used in the social sciences to evaluate the quality of our measurements, and combine different questions or items to form the variables we’re interested in, are factor analysis and reliability analysis. "],["reading-questions-2.html", "2.2 Reading questions", " 2.2 Reading questions Reference Preacher, Kristopher J. and MacCullum, Robert C. (2003) Repairing Tom Swift’s Electric Factor Analysis Machine, Understanding Statistics 2(1) 13-43. Questions What is a latent variable? What is factor analysis and what can you investigate using this method? In the introduction, Preacher and Maccallum talk about a “little jiffy” method of doing factor analysis. Can you shortly list what this litlle jiffy – or bad practice – method of factor analysis is? What are in short the differences between Principal Component Analyses and Exploratory Factor Analyses? What is the purpose of factor rotation? Reference Kestilä, Elina (2006) Is There Demand for Radical Right Populism in the Finnish Electorate? Scandinavian Political Studies 29(3),169-191 Questions What is the research question that the author tries to answer? Describe shortly the characteristics of the Radical Right Parties (RRP) in Europe. What are the two main explanations of support for RRP that this paper focuses on? Does the empirical part of the paper reflect the theoretical framework well? Is Finland very different from other European countries in on the main dependent variables according to the author? What is the conclusion by the author; i.e. what is the answer to the research question? "],["at-home-exercises-1.html", "2.3 At-Home Exercises", " 2.3 At-Home Exercises This exercise is based on: Kestilä, Elina (2006). Is There Demand for Radical Right Populism in the Finnish Electorate? Scandinavian Political Studies 29(3),169-191 You have read and answered questions about the article in the reading questions. In this exercise, as well as in the second class practical, we will analyze these data ourselves. The data for this practical stem from the first round of the European Social Survey (ESS). This is a repeated cross-sectional survey across 32 European countries. The first round was held in 2002, and since then, subsequent rounds of data-collection are held bi- anually. More info, as well as access to all data -&gt; www.europeansocialsurvey.org. The raw, first round data can also be found here. The file is called ESSround1- a.sav. This file contains data for all respondents, but only those variables are included that you will need in this exercise. 2.3.1 Download the file, and import it in R. Inspect the file (no. of cases and no. of variables) to see if the file opened well. For a description of all variables in the dataset, click here! Variable Description name Title of dataset essround ESS round edition Edition proddate Production date cntry Country idno Respondent’s identification number trstlgl Trust in the legal system trstplc Trust in the police trstun Trust in the United Nations trstep Trust in the European Parliament trstprl Trust in country’s parliament stfhlth State of health services in country nowadays stfedu State of education in country nowadays stfeco How satisfied with present state of economy in country stfgov How satisfied with the national government stfdem How satisfied with the way democracy works in country pltinvt Politicians interested in votes rather than peoples opinions pltcare Politicians in general care what people like respondent think trstplt Trust in politicians imsmetn Allow many/few immigrants of same race/ethnic group as majority imdfetn Allow many/few immigrants of different race/ethnic group from majority eimrcnt Allow many/few immigrants from richer countries in Europe eimpcnt Allow many/few immigrants from poorer countries in Europe imrcntr Allow many/few immigrants from richer countries outside Europe impcntr Allow many/few immigrants from poorer countries outside Europe qfimchr Qualification for immigration: christian background qfimwht Qualification for immigration: be white imwgdwn Average wages/salaries generally brought down by immigrants imhecop Immigrants harm economic prospects of the poor more than the rich imtcjob Immigrants take jobs away in country or create new jobs imbleco Taxes and services: immigrants take out more than they put in or less imbgeco Immigration bad or good for country’s economy imueclt Country’s cultural life undermined or enriched by immigrants imwbcnt Immigrants make country worse or better place to live imwbcrm Immigrants make country’s crime problems worse or better imrsprc Richer countries should be responsible for accepting people from poorer countries pplstrd Better for a country if almost everyone share customs and traditions vrtrlg Better for a country if a variety of different religions shrrfg Country has more than its fair share of people applying refugee status rfgawrk People applying refugee status allowed to work while cases considered gvrfgap Government should be generous judging applications for refugee status rfgfrpc Most refugee applicants not in real fear of persecution own countries rfggvfn Financial support to refugee applicants while cases considered rfgbfml Granted refugees should be entitled to bring close family members gndr Gender yrbrn Year of birth edulvl Highest level of education eduyrs Years of full-time education completed polintr How interested in politics lrscale Placement on left right scale 2.3.2 The ESS-file contains much more information than we need to re-analyze the paper by Kestilä. We need to reduce the number of cases to make sure our results pertain to the relevant target population. Kestilä only uses data from ten countries: c(\"Austria\", \"Belgium\", \"Denmark\", \"Finland\", \"France\", \"Germany\", \"Italy\", \"Netherlands\", \"Norway\", \"Sweden\"). As explained in the tutorial chapters at the beginning of this GitBook, it is possible to select rows from the data. In this case, we will select only rows from these countries by means of boolean indexing, using the %in% function to check if the value of $cntry is in the list. Hint: Use []; %in% Click for explanation Use the following code: df &lt;- data[data$cntry %in% c(&quot;Austria&quot;, &quot;Belgium&quot;, &quot;Denmark&quot;, &quot;Finland&quot;, &quot;France&quot;, &quot;Germany&quot;, &quot;Italy&quot;, &quot;Netherlands&quot;, &quot;Norway&quot;, &quot;Sweden&quot;), ] 2.3.3 Inspect the data file again to see whether step 1b went ok. 2.3.4 Before we can start the analyses, we first need to screen the data. What are the things we need to watch for? (think about your earlier statistics-courses)? Click for explanation This question is open to interpretation. One thing you might notice is that all variables the authors used are currently coded as factor variables (e.g., “Factor w/ 11 levels”): library(tidySEM) descriptives(df) levels(df$trstlgl) ## [1] &quot;No trust at all&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; ## [5] &quot;4 &quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; ## [9] &quot;8&quot; &quot;9&quot; &quot;Complete trust&quot; In keeping with conventions, we could treat ordinal Likert scales with &gt;5 levels as continuous. We can either re-code the data, or prevent read.spss() from coding these variables as factors when it reads the data. Here is code for both approaches. 2.3.4.1 Re-coding factors to numeric We can convert a factor to numeric using the function as.numeric(). However, in this case, we have 38 variables to convert - that’s a lot of code. Thankfully, there is a function to apply this transformation to each column of the data: lapply(), short for list apply. This function takes each list element (column of the data.frame), and applies the function as.numeric() to it: df[7:44] &lt;- lapply(df[7:44], as.numeric) 2.3.4.2 Reading data without coding factors An alternative solution is to prevent the function read.spss() from using value labels to code variables as factors when the data are loaded. However, we’re not going to use this right now, so the information below is merely illustrative: # The option use.value.labels = FALSE stops the function from coding factors: data &lt;- read.spss(&quot;ESSround1-a.sav&quot;, to.data.frame = TRUE, use.value.labels = FALSE) # Then, re-select the subset of data. The countries are now also unlabeled, so # we select them by number: df &lt;- data[data$cntry %in% c(21,18,17,15,9,8,6,5,2,1), ] 2.3.5 Aside from screening variables by looking at summary statistics, we can also plot their distributions. You already know how to do this for one single variable. Yet it is not very difficult to plot all variables in a data.frame. To do this, we need to turn the data from “wide format” (one column per variable) into “long format” (one column with the variable names, one column with the values). The package tidyr has a convenient function to reshape data to long format: pivot_longer(). First, make a long data.frame containing variables 7:44. install.packages(&quot;tidyr&quot;) library(tidyr) df_plot &lt;- df[7:44] df_plot &lt;- pivot_longer(df_plot, names(df_plot)) Next, we can plot the data - using geom_histogram(), geom_density() or geom_boxplot() as before. A new additional function allows us to make separate plots for each variable: + facet_wrap(~name, scales = \"free_x\"). Click for explanation library(ggplot2) ggplot(df_plot, aes(x = value)) + geom_histogram() + facet_wrap(~name, scales = &quot;free_x&quot;) Notice the fact that you can see that the scales are actually categorical (because of the gaps between bars), and that most variables look relatively normally distributed despite being categorical. It’s probably fine to treat them as continuous. 2.3.6 Check the scale descriptives table again. Are there any incorrectly coded missing value labels, or other inexplicable values? 2.3.7 The first step in re-analyzing data is replicating the results from the paper by Kestilä. Run a Principal Component Analysis using psych::principal(), and choose the exact same specification as Kestilä concerning estimation method, rotation etc. Do two analyses: one for trust in politics, and one for attitudes towards immigration. Remember that you can view the help file for psych::principal() by running ?psych::principal. Hint: Use psych::principal() Click for explanation 2.3.7.1 Trust in politics Kestilä extracted three components, with VARIMAX rotation. When we print the results, we can hide all factor loadings smaller than the smallest one in their table, to make it easier to read: library(psych) pca_trust &lt;- principal(df[, 7:19], nfactors = 3, rotate = &quot;varimax&quot;) print(pca_trust, cut = .3, digits =3) ## Principal Components Analysis ## Call: principal(r = df[, 7:19], nfactors = 3, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC3 RC2 RC1 h2 u2 com ## trstlgl 0.779 0.669 0.331 1.21 ## trstplc 0.761 0.633 0.367 1.18 ## trstun 0.675 0.556 0.444 1.44 ## trstep 0.651 0.332 0.549 0.451 1.57 ## trstprl 0.569 0.489 0.650 0.350 2.49 ## stfhlth 0.745 0.567 0.433 1.04 ## stfedu 0.750 0.603 0.397 1.14 ## stfeco 0.711 0.300 0.616 0.384 1.44 ## stfgov 0.634 0.377 0.587 0.413 1.88 ## stfdem 0.369 0.568 0.325 0.564 0.436 2.38 ## pltinvt 0.817 0.695 0.305 1.08 ## pltcare 0.811 0.695 0.305 1.11 ## trstplt 0.510 0.611 0.716 0.284 2.40 ## ## RC3 RC2 RC1 ## SS loadings 2.942 2.668 2.490 ## Proportion Var 0.226 0.205 0.192 ## Cumulative Var 0.226 0.432 0.623 ## Proportion Explained 0.363 0.329 0.307 ## Cumulative Proportion 0.363 0.693 1.000 ## ## Mean item complexity = 1.6 ## Test of the hypothesis that 3 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.07 ## with the empirical chi square 15240.94 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.967 For attitude towards immigration, Kestilä extracted five components, with VARIMAX rotation: library(psych) pca_att &lt;- principal(df[, 20:44], nfactors = 5, rotate = &quot;varimax&quot;) print(pca_att, cut = .3) ## Principal Components Analysis ## Call: principal(r = df[, 20:44], nfactors = 5, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC2 RC1 RC5 RC3 RC4 h2 u2 com ## imsmetn 0.80 0.72 0.28 1.3 ## imdfetn 0.78 0.79 0.21 1.7 ## eimrcnt 0.83 0.71 0.29 1.1 ## eimpcnt 0.80 0.79 0.21 1.5 ## imrcntr 0.83 0.75 0.25 1.1 ## impcntr 0.78 0.78 0.22 1.6 ## qfimchr 0.82 0.70 0.30 1.1 ## qfimwht 0.76 0.65 0.35 1.3 ## imwgdwn 0.81 0.71 0.29 1.2 ## imhecop 0.75 0.67 0.33 1.4 ## imtcjob 0.57 0.34 0.48 0.52 2.0 ## imbleco 0.70 0.55 0.45 1.3 ## imbgeco 0.70 0.60 0.40 1.5 ## imueclt 0.57 -0.34 0.54 0.46 2.4 ## imwbcnt 0.67 0.63 0.37 1.9 ## imwbcrm 0.66 0.48 0.52 1.2 ## imrsprc 0.61 0.44 0.56 1.3 ## pplstrd 0.33 -0.54 0.46 0.54 2.2 ## vrtrlg -0.35 0.46 0.41 0.59 2.8 ## shrrfg 0.37 -0.35 0.42 0.58 4.1 ## rfgawrk 0.61 0.40 0.60 1.1 ## gvrfgap 0.69 0.56 0.44 1.3 ## rfgfrpc -0.39 0.33 0.67 3.3 ## rfggvfn 0.58 0.42 0.58 1.5 ## rfgbfml 0.60 0.46 0.54 1.6 ## ## RC2 RC1 RC5 RC3 RC4 ## SS loadings 4.38 3.40 2.78 2.19 1.72 ## Proportion Var 0.18 0.14 0.11 0.09 0.07 ## Cumulative Var 0.18 0.31 0.42 0.51 0.58 ## Proportion Explained 0.30 0.24 0.19 0.15 0.12 ## Cumulative Proportion 0.30 0.54 0.73 0.88 1.00 ## ## Mean item complexity = 1.7 ## Test of the hypothesis that 5 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.05 ## with the empirical chi square 29520.06 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.98 2.3.8 Extract the PCA factor scores from the results objects, and add them to the data.frame. Give the PCA scores informative names, based on your interpretation of the factor loadings, so that you understand what they summarize. Hint: Use $; colnames(); cbind() Click for explanation Extracting factor scores The factor scores are inside of the results objects. Use the $ operator to access them: head(pca_att$scores) ## RC2 RC1 RC5 RC3 RC4 ## 1 1.9920289 1.3140238 -0.8305392 -0.06329775 -0.08837693 ## 4 0.1708174 -1.2167781 -0.4974957 -0.23766146 0.67364069 ## 7 -0.3580985 0.3236336 -1.5094405 -0.53052720 -2.20637993 ## 14 NA NA NA NA NA ## 17 -0.1136716 -0.7869911 -1.4664715 -0.07112144 0.41078167 ## 20 -0.9188606 2.8264230 -0.3477484 -0.73788338 -1.32089442 We’re going to give these factor scores some informative names, and add them to our data.frame. You should give them different, informative names based on the meaning of the factors! # Print names colnames(pca_att$scores) ## [1] &quot;RC2&quot; &quot;RC1&quot; &quot;RC5&quot; &quot;RC3&quot; &quot;RC4&quot; # Change names colnames(pca_att$scores) &lt;- c(&quot;Att1&quot;, &quot;Att2&quot;, &quot;Att3&quot;, &quot;Att4&quot;, &quot;Att5&quot;) colnames(pca_trust$scores) &lt;- c(&quot;Trust1&quot;, &quot;Trust2&quot;, &quot;Trust3&quot;) # Add columns df &lt;- cbind(df, pca_trust$scores, pca_att$scores) 2.3.9 Are you able to replicate her results? Click for explanation No, probably not. The team of teachers was unable to reproduce this analysis, even though it should be pretty easy to do so… 2.3.10 Save your syntax and bring your data and syntax to the practical on Thursday. "],["in-class-exercises-1.html", "2.4 In-Class Exercises", " 2.4 In-Class Exercises In the unlikely event that you were able to replicate the results of Kestilä in the take-home exercise, rerun your analysis script. If you did not manage to replicate the results, load the dataset ESSround1-b.csv into RStudio. 2.4.1 Loading the data If you don’t know how to load the data, click below. Click for explanation 2.4.2 Kestilä states that running a Principal Components Analysis is a good way to test whether the survey questions in the ESS measure attitudes towards immigration and trust in politics. Based on your reading of Preacher and MacCallum (2003), do you agree with this position? 2.4.3 If you would have to choose a method for constructing the ‘trust in politics’ and ‘attitude towards immigration’ scales based on the theory and background information in the Kestilä article, what type of factor analysis would you choose? What key factors influence your decision? Click for information Key factors include: Theory-driven or exploratory? Estimation method Rotation method Method to establish how many factors are needed 2.4.4 Run two factor analyses, one for each PCA of the original article. Inspect the number of factors necessary, evaluate the rotation method, and if necessary, run the factor analysis again with adapted settings (rotation method and/or different number of factors). How many factors are there? Hint: Examing the help file for ?psych::fa Click for explanation To perform an exploratory factor analysis, you can use the function fa of the package psych. You will have to specify the data, and the variables that you want to include in the factor analyses. Furthermore, you will have to specify the number of factors that you want to extract, the rotation method and the estimation method. In order to determine the number of factors to extract, you might want to look at the eigenvalues of the factors, which can be accessed by using the following code: ## [1] 5.054 0.874 0.687 0.304 0.185 0.076 0.014 0.008 -0.060 -0.078 ## [11] -0.123 -0.149 -0.191 Another common approach is to make a scree plot. All that is required for this, is to pass the eigenvalues to the qplot() function and add a geom_path: You can do the same for the attitude variables: ## [1] 8.016 1.702 1.051 0.697 0.500 0.389 0.218 0.162 0.091 0.046 ## [11] 0.043 0.028 0.022 0.008 -0.018 -0.028 -0.032 -0.056 -0.066 -0.083 ## [21] -0.089 -0.119 -0.132 -0.146 -0.266 As recommended in the lecture, a better and more quantified way to establish the number of factors is “parallel analysis” (Horn, 1965). Here’s how to do it in R: ## Parallel analysis suggests that the number of factors = 6 and the number of components = 3 Note that, in this case, parallel analysis recommends as many as 6 factors. However, in order to be able to compare our results with those from Kestilä, we might consider aligning ourselves with the literature and choosing the same number of factors for our analyses as components in the article. 2.4.5 Apart from the number of factors, you also want to look at the factor loadings. They can be found in the “pattern matrix”. The higher the factor loadings are, the more indicative an item is for the latent factor. If you find some items to have only very low loadings (indicating that the items do not provide much information about the factor), you may choose not to include them in your analysis. This means you have to rerun the analysis under question 3. Click for explanation You can find the factor loadings by means of the ‘print’-function used in the take-home exercise, or you can search for the variable ‘loadings’, which is inside the results object, to end up with just the information you are searching for. ## ## Loadings: ## ML1 ML2 ML3 ## pltcare 0.784 -0.126 ## pltinvt 0.783 -0.131 ## trstprl 0.528 0.101 0.258 ## trstlgl 0.827 ## trstplc -0.157 0.800 ## trstplt 0.713 0.116 ## trstep 0.432 0.303 ## trstun 0.343 0.366 ## stfeco 0.128 0.725 -0.129 ## stfgov 0.270 0.640 -0.132 ## stfdem 0.201 0.479 0.129 ## stfedu -0.166 0.665 0.104 ## stfhlth -0.137 0.632 ## ## ML1 ML2 ML3 ## SS loadings 2.518 2.035 1.725 ## Proportion Var 0.194 0.157 0.133 ## Cumulative Var 0.194 0.350 0.483 ## ## Loadings: ## ML3 ML1 ML5 ML2 ML4 ## imsmetn 0.479 0.403 ## imdfetn 0.321 0.563 ## eimrcnt 1.110 -0.187 ## eimpcnt 0.345 0.665 ## imrcntr 0.798 ## impcntr 0.259 0.727 ## qfimchr 0.126 0.867 ## qfimwht 0.114 0.749 ## imwgdwn 0.511 0.182 ## imhecop 0.560 0.153 ## imtcjob 0.712 0.138 ## imbleco 0.702 0.162 ## imbgeco 0.740 ## imueclt 0.470 -0.166 -0.166 ## imwbcnt 0.639 -0.169 ## imwbcrm 0.515 -0.177 0.101 ## imrsprc 0.534 0.159 ## pplstrd 0.245 -0.365 ## vrtrlg -0.115 0.186 0.270 ## shrrfg 0.302 -0.280 ## rfgawrk 0.474 ## gvrfgap 0.752 ## rfgfrpc 0.219 -0.255 ## rfggvfn 0.489 ## rfgbfml 0.632 ## ## ML3 ML1 ML5 ML2 ML4 ## SS loadings 3.271 2.397 2.061 1.549 1.648 ## Proportion Var 0.131 0.096 0.082 0.062 0.066 ## Cumulative Var 0.131 0.227 0.309 0.371 0.437 The matrix of loadings indicates how strongly each factor (columns) is associated with the items (rows). Below the matrix of loadings, we see a second matrix, which indicates (amongst other things) the proportion var: How much variance in the items is explained by each of the factors. Each subsequent factor explains slightly less variance than the ones before it (this is a property of exploratory factor analysis). The cumulative var indicates how much variance the factors explain, in total. If you estimated as many factors as items, then the last value for cumulative var would be 1.00 (100%). The factor loading matrix is slightly hard to read, due to the jumble of factor loadings. To create more clarity, it is convenient to suppress the factor loadings that are lower than .30. ## ## Loadings: ## ML1 ML2 ML3 ## pltcare 0.78 ## pltinvt 0.78 ## trstprl 0.53 ## trstlgl 0.83 ## trstplc 0.80 ## trstplt 0.71 ## trstep 0.43 0.30 ## trstun 0.34 0.37 ## stfeco 0.73 ## stfgov 0.64 ## stfdem 0.48 ## stfedu 0.67 ## stfhlth 0.63 ## ## ML1 ML2 ML3 ## SS loadings 2.52 2.03 1.72 ## Proportion Var 0.19 0.16 0.13 ## Cumulative Var 0.19 0.35 0.48 Furthermore, if you want to perform a factor analysis without, say, stfedu, while you want all other variables included in your factor analysis, you can simply leave the column number of stfedu, which is 13, out of the command: 2.4.6 Give the factor scores an appropriate name. You can do this by inspecting the items that load on one factor. What do these items have in common substantively? The goal of a factor analysis usually is to create interpretable factors. If you have trouble interpreting the factors, you can choose to tweak the analysis by changing the options, or including/excluding more items. Furthermore, after you named the factor scores accordingly, extract them from the results object and add them to the data.frame. Hint: If you do not know how to do this, have a look at question 1.h from the take-home exercise of week 2. Please note that the colnames will be specified from left to right, and not, for example, from ML1 to ML5. 2.4.7 The next step is to assert whether the items that together form one factor, also form a reliable scale. Run separate reliability analyses by means of the function alpha for the items that together form one factor, and evaluate Cronbach’s alpha to see whether the scales are internally consistent. The “Reliability if an item is dropped (alpha.drop)” information may be handy to inspect what would happen if you would delete one item; you can find it inside the reliability analysis object. If Cronbach’s alpha is not ok, deselect one survey item and run the analyses under question 4 and question 5 again. Hint: Cronbach’s alpha &gt; .7 are deemed to be ok, &gt; .8 is good. If Cronbach’s alpha is not ok, deselect one survey item and run the analyses under question 4 and question 5 again. Click for explanation If you want to assess the reliability of the variables pltcare, pltinvt, trstprl, trstplt, and trstep you can run a reliability analysis as follows. Hint: name the new objects substantively, instead of numbering them. ## ## Reliability analysis ## Call: psych::alpha(x = df_with_scores[, c(7, 8, 9, 12, 13)]) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.8 0.83 0.82 0.49 4.8 0.0019 4.4 1.4 0.52 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.8 0.8 0.81 ## Duhachek 0.8 0.8 0.81 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## pltcare 0.79 0.80 0.77 0.50 4.0 0.0021 0.017 0.52 ## pltinvt 0.80 0.81 0.78 0.51 4.2 0.0021 0.016 0.53 ## trstprl 0.73 0.78 0.76 0.47 3.6 0.0027 0.018 0.52 ## trstplt 0.70 0.75 0.73 0.43 3.1 0.0031 0.014 0.43 ## trstep 0.78 0.82 0.80 0.53 4.6 0.0021 0.010 0.52 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## pltcare 17975 0.66 0.75 0.67 0.56 2.6 1.1 ## pltinvt 17971 0.64 0.74 0.65 0.54 2.4 1.1 ## trstprl 17753 0.84 0.80 0.74 0.69 6.3 2.3 ## trstplt 17966 0.88 0.86 0.83 0.77 5.3 2.2 ## trstep 16390 0.77 0.70 0.59 0.57 5.6 2.3 Sometimes, the table “Reliability if an item is dropped” will indicate that Cronbach’s alpha increases when you drop a variable out of the analysis. Note, however, that doing so is an exploratory approach to analysis, and it may make your work incomparable to other publications using the same scale. 2.4.8 Now you can analyze the differences between the factor scores for the PCA analysis (take-home exercise 2) and the EFA by plotting them in a series of scatterplots (bivariate) using ggplot2. The PCA factor scores are already stored in the dataset ESSround1-b.csv in columns 45 through 52 (df[, 45:52]). Plot the scores for one of your new factors on the x axis, and try to match it with a corresponding PCA component in the dataset. Note that if you went with a different number of factors than Kestilä’s components (5 for ‘trust in politics’ and 3 for ‘attitudes towards immigration’), you may find little correlation between any factors. Click for explanation Make sure to adjust the arguments x = EFA_trust1 and y = PCA_Trust1 to align with your own EFA factor column name and a PCA component column name from the data respectively 2.4.9 Examine the correlations between the PCA and EFA scales for the ‘trust in politics’ scores, and for the ‘immigration’ scores. What is your conclusion: is there a difference between them? Click for explanation Hint: Name the new objects substantively once again. Example for trust in politics: ## Call:corr.test(x = df_with_scores$trustinstEFA, y = df_with_scores$trustinst, ## use = &quot;complete.obs&quot;) ## Correlation matrix ## [1] 0.94 ## Sample Size ## [1] 14778 ## These are the unadjusted probability values. ## The probability values adjusted for multiple tests are in the p.adj object. ## [1] 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option 2.4.10 Kestila uses the PCA factor scores to evaluate country level differences in 1. Attitudes towards immigration and 2. Political trust. Repeat her analyses using the factor scores you saved in step 5. Think about the statistical test you would like to use. Do you draw similar or different conclusions? Click for explanation We can use an ANOVA to test whether the countries differ in the amount of political trust the participants have. ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cntry 8 1740 217.55 199.6 &lt;2e-16 *** ## Residuals 14769 16100 1.09 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 3409 observations deleted due to missingness In which trustinstEFA is the dependent variable, cntry is the grouping variable, and df is the name of the dataset; summary will provide the results. However, it will only indicate whether the variable cntry is significant or not, so you will be unable to tell which countries differ in terms of their political trust. To tell which countries differ, you can do a pairwise comparison test, with a Bonferroni adjustment for multiple testing. ## Austria Belgium Denmark Finland Germany Italy Netherlands Norway ## Belgium 0.000 NA NA NA NA NA NA NA ## Denmark 0.000 0 NA NA NA NA NA NA ## Finland 0.000 0 0.085 NA NA NA NA NA ## Germany 0.033 0 0.000 0 NA NA NA NA ## Italy 1.000 0 0.000 0 1 NA NA NA ## Netherlands 0.000 0 0.000 0 0 0.000 NA NA ## Norway 0.000 0 0.000 0 0 0.000 0 NA ## Sweden 0.231 0 0.000 0 0 0.144 0 0.005 2.4.11 The second goal of Kestilä is to show how socio-demographic characteristics affect attitudes towards immigrants and trust in politics in Finland. Select only the Finnish cases using the variable cntry. Click for explanation To select the Finnish cases only, select rows for which $cntry is equal to (==) Finland: After selecting the Finnish cases, we have to prepare our variables for analysis. Thanks to the factor analyses above, our dependent variables are taken care of. However, we should examine the independent variables to make sure there are no surprises, and do any recoding necessary before we can run multiple regression. The independent variables are c(\"gndr\", \"yrbrn\", \"eduyrs\", \"polintr\", \"lrscale\"). Click for explanation Note that we still have to do some recoding. For example, the data contains participants’ year of birth instead of their age. We have to recode this variable. The data were collected in 2002, so we can simply subtract the year of birth of every participant from the year 2002. Furthermore, the variables polintr and lrscale are currently coded as character variables. If you analyze them like that, R will make dummies for all distinct values on these variables. Let’s recode those, too! The main challenge is to get the levels of these variables in the correct order, and convert them to numbers. We demonstrate two ways to do this, one way for each of the two variables. For political interest, we convert the character variable to an ordered categorical variable, and we specify the correct order of labels. Then, we convert it to a numeric variable. ## ## Hardly interested Not at all interested Quite interested ## 842 228 785 ## Very interested ## 144 ## ## Not at all interested Hardly interested Quite interested ## 228 842 785 ## Very interested ## 144 For political orientation, only the two extreme values of the scale are labeled as text. Thus, we can replace these labels with numbers, and then convert the entire variable to numeric. ## ## 1 2 3 4 5 6 7 8 9 Left Right ## 25 62 148 183 599 199 296 217 79 24 59 Furthermore, Kestilä recoded polintr in such a way that there are only two categories. The lowest two and highest two categories were combined. Here is how to do this in R, so you can replicate their analysis: Next, run a number of multiple linear regression analyses with the (sub-)scales of attitudes towards immigrants and political trust as subsequent dependent variables, and the same predictors as Kestilä. Inspect your output. Compare your results with the results from Kestilä. How do your results differ or agree with the results by Kestilä? ## ## Call: ## lm(formula = trustinstEFA ~ gndr + age + eduyrs + polintr_dummy + ## lrscale, data = df_finland, na.action = na.omit) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.0364 -0.4740 0.1394 0.6379 2.3394 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.100441 0.125066 -0.803 0.4220 ## gndrMale 0.010228 0.044386 0.230 0.8178 ## age -0.001053 0.001356 -0.776 0.4377 ## eduyrs 0.027904 0.006277 4.445 9.34e-06 *** ## polintr_dummyQuite or very interested 0.090948 0.045642 1.993 0.0465 * ## lrscale 0.051650 0.011037 4.680 3.10e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9159 on 1734 degrees of freedom ## (260 observations deleted due to missingness) ## Multiple R-squared: 0.03377, Adjusted R-squared: 0.03099 ## F-statistic: 12.12 on 5 and 1734 DF, p-value: 1.454e-11 2.4.12 Save your syntax and your data, you will need it next week. "],["formative-assessment-1.html", "2.5 Formative Assessment", " 2.5 Formative Assessment A formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material. Complete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention. Question 1: With PCA, you must indicate the number of factors to extract a-priori. FALSE TRUE Question 2: When conducting an exploratory factor analysis, cross-loadings are assumed to be zero. TRUE FALSE Question 3: Order these methods for determining the number of factors from least subjective to most subjective: 1) Kaiser’s criterion, 2) Scree plot, 3) Parallel analysis. 3, 2, 1 1, 2, 3 3, 1, 2 2, 1, 3 Question 4: What kind of model is displayed above? Exploratory Factor Analysis Confirmatory Factor Analysis An Item Response Theory Analysis Principal Components Analysis Question 5: EFA models the covariances among items; PCA describes the total variance of items. FALSE TRUE Question 6: Factor analysis is a statistical technique aimed at data reduction. TRUE FALSE Question 7: What is the maximum number of latent variables when conducting Exploratory Factor Analysis with 5 items? No limit; you can keep adding latent variables, but they will explain ever decreasing amounts of variance. Two; you need 3 indicators for one latent variable, but the model is still identified due to the covariance between latent variables. Five; the same as the number of indicators. Only one; you need at least 3 indicators per latent variable. Question 8: Promax rotation in factor analysis is a specific form of orthogonal rotation. TRUE FALSE Question 9: Which of these is not a valid method for choosing the correct number of factors? Kaiser’s criterion Ability to predict a criterion scale. Scree plot Parallel analysis Theory Question 10: Varimax is a form of orthogonal rotation in factor analysis TRUE FALSE "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
