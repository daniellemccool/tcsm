[["index.html", "Theory Construction and Statistical Modeling Course Information", " Theory Construction and Statistical Modeling Kyle M. Lang Last updated: 2022-09-14 Course Information In order to test a theory, we must express the theory as a statistical model and then test this model on quantitative (numeric) data. In this course we will use datasets from different disciplines within the social sciences (educational sciences, psychology, and sociology) to explain and illustrate theories and practices that are used in all social science disciplines to statistically model social science theories. This course uses existing tutorial datasets to practice the process of translating verbal theories into testable statistical models. If you are interested in the methods of acquiring high quality data to test your own theory, we recommend following the course Conducting a Survey which is taught from November to January. Most information about the course is available in this GitBook. Course-related communication will be through https://uu.blackboard.com (Log in with your student ID and password). "],["acknowledgement.html", "Acknowledgement", " Acknowledgement This course was originally developed by dr. Caspar van Lissa. Indeed, you will still see Caspar in the lecture recordings. I (dr. Kyle M. Lang) have modified Caspar’s original materials and take full responsibility for any errors or inaccuracies introduced through these modifications. Credit for any particularly effective piece of pedagogy should probably go to Caspar. You can view the original version of this course here on Caspar’s GitHub page. "],["instructors.html", "Instructors", " Instructors Coordinator: dr. Kyle M. Lang Lectures: dr. Kyle M. Lang Practicals: Rianne Kraakman Daniëlle Remmerswaal Laura Jochim "],["course-overview.html", "Course overview", " Course overview This course comprises three parts: Factor analysis: You will learn different ways of defining and estimating unobserved constructs. Path analysis: You will learn how to conduct regressions and ANOVAs as structural equation models with observed variables. Full structural equation modeling: You will combine the first two topics by estimating path models using latent variables. Each of these three themes will be evaluated with a separate assignment. Your course grade will be based on the weighted average of these three assignment grades. Schedule Course Week Calendar Week Topic Assignment 0 36 Preperation, Working w/ R 1 37 Introduction to statistical modeling 2 38 Exploratory factor analyis (EFA) 3 39 Confirmatory factor analysis (CFA) 4 40 N/A Project 1 Due 5 41 General linear model (GLM), Structural equation modeling (SEM) 6 42 Mediation 7 43 Moderation Project 2 Due 8 44 Full SEM 9 45 N/A Project 3 Due "],["learning-goals.html", "Learning goals", " Learning goals In this course you will learn how to translate a social scientific theory into a statistical model, how to analyze your data with these models, and how to interpret and report your results following APA standards. After completing the course, you will be able to: Translate a verbal theory into a conceptual model, and translate a conceptual model into a statistical model. Independently analyze data using the free, open-source statistical software R. Apply a latent variable model to a real-life problem wherein the observed variables are only indirect indicators of an unobserved construct. Use a path model to represent the hypothesized causal relations among several variables, including relationships such as mediation and moderation. Explain to a fellow student how structural equation modeling combines latent variable models with path models and the benefits of doing so. Reflect critically on the decisions involved in defining and estimating structural equation models. "],["resources.html", "Resources", " Resources Literature You do not need a separate book for this course! Most of the information is contained within this GitBook and the course readings (which you will be able to access via links in this GitBook). All literature is freely available online, as long as you are logging in from within the UU-domain (i.e., from the UU campus or through an appropriate VPN). All readings are linked in this GitBook via either direct download links or DOIs. If you run into any trouble accessing a given article, searching for the title using Google Scholar or the University Library will probably due the trick. Software You will do all of your statistical analyses with the statistical programming language/environment R and the add-on package lavaan. If you want to expand your learning, you can follow this excellent lavaan tutorial. Doing so is entirely optional, though. "],["reading-questions.html", "Reading questions", " Reading questions Along with every article, we will provide reading questions. You will not be graded on the reading questions, but it is important to prepare the reading questions before every lecture. The reading questions serve several important purposes: Provide relevant background knowledge for the lecture Help you recognize and understand the key terms and concepts Make you aware of important publications that shaped the field Help you extract the relevant insights from the literature "],["weekly-preparation.html", "Weekly preparation", " Weekly preparation Before every class meeting (both lectures and practicals) you need to do the assigned homework (delineated in the GitBook chapter for that week). This course follows a flipped classroom procedure, so you must complete the weekly homework to meaningfully participate in, and benefit from, the class meetings. Background knowledge We assume you have basic knowledge about multivariate statistics before entering this course. You do not need any prior experience working with R. If you wish to refresh your knowledge, we recommend the chapters on ANOVA, multiple regression, and exploratory factor analysis from Field’s Discovering Statistics using R. If you cannot access the Field book, many other introductory statistics textbooks cover these topics equally well. So, use whatever you have lying around from past statistics courses. You could also try one of the following open-access options: Applied Statistics with R Introduction to Modern Statistics Introduction to Statistical Learning "],["grading.html", "Grading", " Grading Your grade for the course is based on a “portfolio” composed of the three take-home assignments: Latent variable modeling Deadline: 2022-10-07 23:59 Group assignment Contributes 25% of your course grade Path modeling Deadline: 2022-10-28 23:59 Group assignment Contributes 25% of your course grade Full Structural equation modeling Deadline: 2022-11-11 23:59 Individual assignment Contributes 50% of your course grade The specifics of the assignments are explicated in the Assignments chapter of this GitBook "],["attendance.html", "Attendance", " Attendance Attendance is not mandatory, but we strongly encourage you to attend all lectures and practicals. In our experience, students who actively participate tend to pass the course, whereas those who do not participate tend to drop out or fail. The lectures and practicals build on each other, so, in the unfortunate event that you have to miss a class meeting, please make sure you have caught up with the material before the next session. "],["assignments.html", "Assignments", " Assignments This chapter contains the details and binding information about the three assignments that comprise the portfolio upon which your course grade is based. For each assignment, you will use R to analyze some real-world data, and you will write up your results in a concise report. The required components of these analyses/reports are delineated in the following three sections. You will submit this report via Blackboard. You will complete the first two assignments in your Assignment Group. You will complete the third assignment individually. The first two assignments each contribute 25% of your course grade. The third assignment contributes 50% of your course grade. "],["assignment-1-latent-variable-model.html", "Assignment 1: Latent Variable Model", " Assignment 1: Latent Variable Model In the first assignment, you will work in groups to apply a latent variable model to a real-world problem wherein the observed variables are indirect indicators of an unobserved social scientific construct. The components of the first assignment are described below. Find a suitable dataset, and describe the data. (150 words). Data you have collected yourself (e.g., for a previous course) Open data (e.g., data provided with a published paper) The “Coping with COVID-19” dataset (if you can’t find anything else) State the research question, and define the theoretical latent variable model. (150 words) Be sure to clearly explicate all parts of your model. Translate your theoretical latent variable model into lavaan syntax, and estimate the model. Provide relevant output in a suitable format. Explain your rationale for important modeling decisions. (300 words) Motivate your choice of latent variable model (i.e., EFA, CFA, PCA). Discuss assumptions. Discuss other important decisions that could have influence your results. Report and interpret the results in APA style. Include measures of model fit. Discuss the results. (300 words) Use your results to answer the research question. Consider the strengths and limitations of your analysis. Submission Assignment 1 is due at 23:59 on Friday 7 October 2022 Submit your report via the Assignment 1 portal on Blackboard. "],["assignment-2-path-model.html", "Assignment 2: Path Model", " Assignment 2: Path Model For the second assignment, you will work in groups to apply a path model that describes how several variables could be causally related. The components of the second assignment are described below. Find a suitable dataset, and describe the data. (150 words) Data you have collected yourself (e.g., for a previous course) Open data (e.g., data provided with a published paper) The “Coping with COVID-19” dataset (if you can’t find anything else) State the research question, and define the theoretical path model. (150 words) This model can be a re-analysis of a question that was originally tested using regression or ANOVA in a published paper. Translate your theoretical path model into lavaan syntax, and estimate the model. Provide relevant output in a suitable format. Explain your rationale for important modeling decisions. (300 words) Discuss the conceptual fit between your theory and your model. Evaluate the model assumptions. If applicable, discuss the differences and similarities between your path model and the regression or ANOVA results from the original paper. Discuss other important decisions that could have influence your results. Report and interpret the results in APA style. Include measures of explained variance for the dependent variables. Discuss the results. (300 words) Use your results to answer the research question. Consider the strengths and limitations of your analysis. Submission Assignment 2 is due at 23:59 on Friday 28 October 2022 Submit your report via the Assignment 2 portal on Blackboard. "],["assignment-3-full-structural-equation-model.html", "Assignment 3: Full Structural Equation Model", " Assignment 3: Full Structural Equation Model In the third assignment, you will work individually to apply a full SEM that describes how several (latent) variables could be causally related. The components of the third assignment are described below. Find a suitable dataset, and describe the data. (150 words) Data you have collected yourself (e.g., for a previous course) Open data (e.g., data provided with a published paper) The “Coping with COVID-19” dataset (if you can’t find anything else) State the research question, and define the theoretical SEM. (200 words) This model can be a re-analysis of a question that was originally tested using regression or ANOVA in a published paper. The structural component of this model must include, at least, three variables. The model must include, at least, one latent variable. Translate your theoretical SEM into lavaan syntax, and estimate the model. Provide relevant output in a suitable format. Explain your rationale for important modeling decisions. (300 words) Discuss the conceptual fit between your theory and your model. Evaluate the model assumptions. If applicable, discuss the differences and similarities between your path model and the regression or ANOVA results from the original paper. Discuss other important decisions that could have influence your results. Report and interpret the results in APA style. Include measures of model fit. Include measures of explained variance for the dependent variables. Discuss the results. (500 words) Use your results to answer the research question. Consider the strengths and limitations of your analysis. Submission Assignment 3 is due at 23:59 on Friday 11 November 2022 Submit your report via the Assignment 3 portal on Blackboard. "],["data.html", "Data", " Data If you have your own data, that’s great, and we strongly encourage you to use those data for the projects. That being said, most students will not have their own data. If you’re one of the majority without data, you’ll have to find an existing dataset to analyze (in case you were wondering, you won’t have time to collect new data). Sources Those who need data can make use of one of the myriad publicly available datasets available online. Of course these datasets will vary widely in terms of their applicability to the project and the amount of work required to prepare the variables for analysis. The following links point to various sources of publicly available data. Kaggle datasets GSS Sata Explorer StatLine Open Data Open Data from the Dutch Government Google Cloud Platform Google dataset search Tips When searching for a suitable dataset, consider the following tips: It will take longer than you expect. Start searching for data early. Plan on devoting a good chunk of time to tracking down a useful dataset. Realize that even after you find the data, you will also have to do a good bit of cleaning to get the variables ready for analysis. Expect to be disappointed. You may find a dataset that looks good on paper but won’t work for the project (e.g., too much missing data, poor coverage for key relations, jenky distributions, etc.). You usually won’t discover these issue until you’ve already downloaded the data and started exploring/cleaning. Make sure you check the actual data you’ve downloaded before you declare success in your search. Keep the project requirements in mind when searching for data. You will need, at least, three interesting variables to model in some sort of causal process (required for Assignment 3). You will need, at least, one latent variable (required for Assignments 1 &amp; 3). To define a latent variable, you need multiple items (at least three) measuring the same hypothetical construct. In practice, these multiple indicators usually come from scales (e.g., NEO-PI Big Five Personality Inventory, Beck Depression Inventory). If you find a dataset that looks suitable, but you want to confirm, send me (Kyle) a link to the dataset, and I’ll let you know if it will work for the assignments. "],["procedures.html", "Procedures", " Procedures Formating You must submit your assignment reports in PDF format. Each report should include a title page. The title page should include the following information: The name of the assignment. The names of all assignment authors (i.e., all group members for Assignments 1 &amp; 2, your name for Assignment 3). The Assignment Group number (only for Assignments 1 &amp; 2). Length In the preceding three sections, some line-items in the descriptions of the assignments concluded with a word count. These sections of your report must conform to the indicated word counts. E.g., the Data section of Assignment 1 must contain no more that 150 words, and the Discussion section of Assignment 3 must contain no more than 500 words. In all other sections, you may use as many words as necessary to adequately explain yourself (though concision and parsimony are still encouraged). Note that the assignments are not intended to be full-blown papers! You only get a few hundred words to describe your data, justify your theoretical model, and discuss the results. The focus should be on the definition of your model, how this model relates to theory (introduction), and what you have learned from your estimated model (discussion). Submission You will submit your reports through Blackboard. Each assignment has a corresponding item in the “Assignments” section of the BB page through which you will submit your reports. For Assignments 1 &amp; 2, you may only submit one report per group. Designate one group member to submit the report. The grade for this submission will apply to all group members. If something goes wrong with the submission, or you notice a mistake (before the deadline) that you want to correct, you may upload a new version of your report. We will grade the final submitted version. The submissions will be screened with SafeAssign. "],["grading-1.html", "Grading", " Grading Each assignment comprises 6 components (i.e., the line-items labelled 1–6 in the assignment descriptions). Each of these 6 elements will be scored as: Insufficient (0 points) Sufficient (1 point) Excellent (1.5 points) The individual assignment grades will be computed as follows: Sum the 6 component grades. Add 1 to the sum of the component grades. Consequently, we have the following landmarks: All components marked as Insufficient \\(\\rightarrow\\) Assignment Grade = 1 All components marked as Sufficient \\(\\rightarrow\\) Assignment Grade = 7 All components marked as Excellent \\(\\rightarrow\\) Assignment Grade = 10 The final course grade, \\(G_{course}\\), will be computed as the weighted average of the three individual assignment grades, \\(G_{a1}\\), \\(G_{a2}\\), \\(G_{a3}\\): \\[ G_{course} = 0.25 G_{a1} + 0.25 G_{a2} + 0.5 G_{a3} \\] "],["rules.html", "Rules", " Rules Resources For all three assignments, you may use any reference materials you like, including: All course materials The course GitBook Additional books and papers The internet Collaboration You will complete the first two assignments in groups. Although you will work in groups, your group may not work together with other groups. You will complete the final assignment individually. For this assignment, you may not work with anyone else. For all three assignments, you are obligated to submit original work (i.e., work conducted for this course by you or your group). Submitting an assignment that violates this condition constitutes fraud. Such cases of fraud will be addressed according to the University’s standard policy. Academic integrity Hopefully, you also feel a moral obligation to obey the rules. For this course, we have implemented an examination that allows you to showcase what you have learned in a more realistic way than a written exam would allow. This assessment format spares you the stress of long exams (the two exams for this course used to be 4 hours each) and the attendant studying/cramming. The assignments will also help you assess your ability to independently analyse data, which is important to know for your future courses and/or career. However, this format also assumes that you complete the assignments in good faith. So, I simply ask that you hold up your end of the bargain, and submit your original work to show us what you’ve learned. Strict stuff By submitting your assignments (both group and individual), you confirm the following: You have completed the assignment yourself (or with your group) You are submitting work that you have written yourself (or with your group) You are using your own UU credentials to submit the assignment You have not had outside help that violates the conditions delineated above while completing the assignment All assignments will be submitted via SafeAssign in Blackboard and, thereby, checked for plagiarism. If fraud or plagiarism is detected or suspected, we will inform the Board of Examiners in the usual manner. In the event of demonstrable fraud, the sanctions delineated in Article 5.15 of the Education and Examination Regulations (EER) will apply. "],["software-setup.html", "Software Setup", " Software Setup This chapter will help you prepare for the course by showing how to install R and RStudio on your computer. If you’re already using R, there may be nothing new for you here. That being said, you should look over this chapter to ensure that your current setup will be compatible with the course requirements. If you have never used R before, this chapter is essential! The information is this chapter will be crucial for getting your computer ready for the course. "],["typographic-conventions.html", "0.1 Typographic Conventions", " 0.1 Typographic Conventions Throughout this GitBook, we (try to) use a consistent set of typographic conventions: Functions are typeset in a code font, and the name of the function is always followed by parentheses E.g., sum(), mean() Other R objects (e.g., data objects, function arguments) are in also typeset in a code font but without parentheses E.g., seTE, method.tau Sometimes, we’ll use the package name followed by two colons (::, the so-called *scope-resolution operator), like lavaan::sem(). This command is valid R code and will run if you copy it into your R console. The lavaan:: part of the command tells R that we want to use the sem() from the lavaan package. "],["installing-software.html", "0.2 Installing software", " 0.2 Installing software Before we start the course, we have to install three things: R: A free program for statistical programming RStudio: An integrated development environment (IDE) which makes it easier to work with R. Several packages: Separate pieces of ‘add-on’ software for R with functions to do specific analyses. Packages also include documentation describing how to use their functions and sample data. 0.2.1 Installing R The latest version of R is available here. Click the appropriate link for your operating system and follow the instructions for installing the latest stable release. Depending on which OS you select, you may be given an option to install different components (e.g., base, contrib, Rtools). For this course, you will only need the base package. 0.2.2 Installing RStudio Download the Free Desktop version of RStudio from the download page of the RStudio website. 0.2.3 Installing packages To participate in this course, you will need a few essential R packages. Here’s an overview of the packages and why we need them: Package Description lavaan A sophisticated and user-friendly package for structural equation modeling ggplot2 A flexible and user-friendly package for making graphs tidySEM Plotting and tabulating the output of SEM-models semTools Comparing models, establishing measurement invariance across groups psych Descriptive statistics foreign Loading data from SPSS ‘.sav’ files readxl Loading data from Excel ‘.xslx’ files To install these packages, we use the install.packages() function in R. Open RStudio Inside RStudio, find the window named Console on left side of the screen. Copy the following code into the console and hit Enter/Return to run the command. install.packages(c(&quot;lavaan&quot;, &quot;ggplot2&quot;, &quot;tidySEM&quot;, &quot;semTools&quot;, &quot;psych&quot;, &quot;foreign&quot;, &quot;readxl&quot;), dependencies = TRUE) "],["getting-the-course-data.html", "0.3 Getting the course data", " 0.3 Getting the course data All of the data files you will need for the course are available in this SurfDrive directory. Follow the link to download a ZIP archive containing the data you will need to complete the practical exercises. Extract these data files to a convenient location on your computer. "],["r-tutorial.html", "R Tutorial", " R Tutorial Welcome to the world of R! This chapter provides a tutorial based on “R: How to get started” by Ihnwhi Heo, Duco Veen, and Rens van de Schoot. If you already have a good deal of R experience, and you don’t see anything new in this tutorial, feel free to skip this chapter. If you haven’t worked with R before, I strongly encourage you to spend some time carefully working through this tutorial. The content of this chapter will go a long way towards familiarizing you with the basic R commands that you will need to use at the beginning of the course. "],["who-r-you.html", "0.4 Who R you?", " 0.4 Who R you? R is a piece of free software for statistical computation and graphics. R is also fully open-source, which means anyone (even you!) can improve, develop, and contribute to R You can find the official manual from the R Core Team here: An introduction to R R itself looks a bit old-fashioned and tedious: "],["rstudio.html", "0.5 RStudio", " 0.5 RStudio Thankfully, we have a great user interface for R called RStudio! RStudio helps you use and learn R more easily. Although you are interacting with RStudio, you are still using R. Don’t ever write something like, “We conducted all analysis with RStudio…” For this course, all tutorials and practicals will use RStudio. 0.5.1 No ‘pane’, no gain! When you open RStudio, the screen should look something like the following image. You will notice that the window is divided into ‘panes’ (a pane is a division of a window). Before we explain these three panes, I want you to add another, which you will see if you open an R script. An R script is simply a plain-text file wherein you will write the R code that executes your analyses. When you open an R script (or create a new one, which is really the same thing), a fourth pane appears. 0.5.2 Create a new R script To create a new R script, Click the icon with a plus sign on the paper (highlighted below by the red square). When you click this icon, a new script is generated and appears in a fourth pane on the upper left side of the screen (if you’re using the default layout). Note that this script is not yet saved anywhere. If you close this script without saving, you will lose all its contents. The four panes help organize your workflow. Rstudio is an integrated development environment (IDE) that is meant to integrate all of the tasks you may need to do for any R-based analysis, programming, or development. Hence, in RStudio, the intended workflow will have you do everything in one window, and the four panes make this workflow more efficient (in theory, at least). 0.5.3 What do the four panes do? Note that the following descriptions apply to the default layout. You can change the orientation and content of the panes (although you must always have four). Source Pane: Located in the top left quadrant. This pane is also called the “Editor”, because this is where we edit scripts. We will usually type our code in the source pane. Console Pane: Located in the bottom left quadrant. This pane is for direct communication with R. We can type commands here that are immediately evaluated (whereas the commands in a script are only evaluated when we explicitly run them). Furthermore, all output from the R commands that we run (either via the console or a script) is printed in the console pane. The two panes on the right side of the window contain various tabs. Two of these tabs are especially useful. Environment Tab: Located in the upper right quadrant The environment tab contains all the ‘objects’ currently loaded in your R session. You can always check what objects are loaded under the environment tab. The ‘environment’ is also called the ‘workspace’ Plots Tab: Located in the lower right quadrant The plots tab shows any graphs and figures we draw via R commands. If you zoom by clicking the magnifying glass icon, you can see enlarged versions of the plots. "],["rstudio-projects.html", "0.6 RStudio projects", " 0.6 RStudio projects To keep all your work organized, you can use an RStudio Project. One advantage of using RStudio projects is that the project directory is automatically set as the working directory. If you save your data in the directory that contains the “.Rproj” file, you will can load the data without specifying the file path. 0.6.1 Starting a new project in Rstudio In Rstudio, click on the New project button: In the pop-up dialog, click New Directory. Click New Project. Type the desired directory name in the dialog box Give a meaningful name, e.g., “TCSM_Course”. Use the Browse button if you need to change the location of the directory that you will use to store this project. "],["loading-data.html", "0.7 Loading data", " 0.7 Loading data Statistical analysis cannot happen without data. In R, you can load data in various ways. Let’s go over a couple of these. To complete the following exercise, first download the LifeSat.sav data. 0.7.1 Via clicky-box options Click through the following menu options: File &gt; Import Dataset Choose the type of dataset. For this exercise, the data are stored as an SPSS .sav file. Thus, select the From SPSS option. You may encounter an Install Required Packages pop-up dialog with a message that asks you whether you want to install the haven package. A package is an a piece of add-on software for R. To do most analyses, you will need some packages above-and-beyond the Base R software. In this case, haven contains functions that allow R to read SPSS data files. Click ‘Yes’ to install the package. At this point, the Import Statistical Data pop-up dialog should appear. Provide the location of your data file in the File/URL field. You can type the file path directly. You can also click Browse to find the file via a GUI interface. You should now see your data in the Data Preview area. In the Import Options section, you can set the name and format of your data file. All of your your selections are being translated to the R-code required to load your file. In the Code Preview area, the steps required to load the data with your selected options are expressed in terms of code. Finally, click Import at the lower right side of the window to load your data. 0.7.2 Via R code Of course, we don’t have to use tedious clicky-box processes. Once you get more familiar with R, it will be much easier to load data with R code. For this exercise, we will use the read.spss() function from the foreign package. Copy the following code into an R-script. Note that this code assumes you have saved your data in the working directory (this is the project directory if you’re using an R project). Otherwise, you will need to specify the file path to your data file as the first argument. library(foreign) LifeSat &lt;- read.spss(&quot;LifeSat.sav&quot;, to.data.frame = TRUE, use.value.labels = FALSE) To run the code: Place your cursor on the first line of code. Press the Ctrl/Cmd + Enter keys together. The code should now be evaluated by R, and R will tell you the result of its actions in the console pane. 0.7.2.1 Data frames When we read data into R, the data are most often stored as a special type of object called a data frame. Data frames are the preferred way to handle data in R. Data frames can hold variables with different types (e.g., a numeric depression rating, a categorical grouping factor representing employment status, and a character string recording the response to an open-ended question). The option to.data.frame = TRUE in the read.spss() call above loads the data into a data frame. 0.7.3 From Excel files We can also load data that are stored in Excel files. One way to do so is via the readxl package. ## Load the package: library(readxl) ## Read the first sheet of &#39;LifeSat.xlsx&#39; into &#39;LifeSat&#39;: LifeSat &lt;- read_excel(&quot;LifeSat.xlsx&quot;, sheet = 1) "],["exploring-data-via-r-functions.html", "0.8 Exploring data via R functions", " 0.8 Exploring data via R functions If you look a bit more closely at the R code that we used to load the data, you will see two special commands: library() and read.spss(). These commands are called functions. Any R command that is written as a name followed by parentheses, e.g., mean(), is a function. Functions are the driving force behind all R-based data analysis. Functions tell R to perform a specific (potentially very complicated) task. Rather than having to write out along sequence of commands every time we want to do some task, we can simply call the appropriate function. When using functions, you need to provide appropriate inputs to specify the behavior of the function and give the function data on which to operate. These inputs are called function arguments. Let’s explore three new functions and their arguments. These functions can help us understand our data. 0.8.1 head() Our data comprise many rows. We can use the head() function to inspect the first several rows. To use the head() function, you only need to provide one argument: The name of the dataset. ## Inspect the first several rows head(LifeSat) “Wait, what is the hash tag (#) doing there?” Don’t be surprised. The hash tag creates a “comment”. I.e., a bit of text that will not be evaluated by R. Comments let us write notes to explain what a particular piece of code does. Comments are doubly useful. They can help others understand your code. They can also help you remember what the code does after some time away. Copy the preceding code into your R script and run it. If all went well, R should now show the first six rows of the dataset in the console. 0.8.2 str() We frequently want to know something about the types of variables in a dataset. For example, if you want to run an analysis of variance (ANOVA), the independent variable(s) should be categorical. In R, these variables would be represented by a special type of variable called a factor. Before running our analysis, we should check if are data satisfy this requirement. We can use the str() function to get some information about the structure of an R object (str is an abbreviation of structure). To run the str() function, you only need to provide one argument: the name of the dataset. ## Inspect the structure of the dataset str(LifeSat) ## &#39;data.frame&#39;: 98 obs. of 8 variables: ## $ LifSat : num 13 18 19 24 24 24 30 33 33 33 ... ## $ age : num 75 75 72 72 70 73 72 72 68 73 ... ## $ educ : num 6 5 5 6 5 6 6 5 7 6 ... ## $ gender : num 2 2 2 2 1 2 1 2 1 1 ... ## ..- attr(*, &quot;value.labels&quot;)= Named chr [1:2] &quot;2&quot; &quot;1&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;male&quot; &quot;female&quot; ## $ female : num 0 0 0 0 1 0 1 0 1 1 ... ## $ ChildSup: num 4 6 6 6 6 8 8 7 4 8 ... ## $ SpouSup : num 2 5 5 4 5 6 4 6 2 8 ... ## $ SES : num 3 1 1 1 1 1 1 2 2 2 ... ## ..- attr(*, &quot;value.labels&quot;)= Named chr [1:3] &quot;3&quot; &quot;2&quot; &quot;1&quot; ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;high SES&quot; &quot;middle SES&quot; &quot;low SES&quot; ## - attr(*, &quot;variable.labels&quot;)= Named chr [1:8] &quot;&quot; &quot;&quot; &quot;Years of education&quot; &quot;&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;LifSat&quot; &quot;age&quot; &quot;educ&quot; &quot;gender&quot; ... ## - attr(*, &quot;codepage&quot;)= int 1252 According to the output printed to the console, the LifeSat dataset is a data frame consisting of 98 observations of 8 variables. I.e., our dataset has 98 rows and 8 columns. We also get information about the contents of each column. For the fist column, we see $ LifSat : num 13 18 19 24 24 24 30 33 33 33 ... The LifSat variable is numeric (abbreviated as num). 13 is the value in the first row, 18 is the value in the second row, 19 is the value in the third row, and so on. 0.8.3 summary() We can get the descriptive statistics of the variables in a dataset with the summary() function. Again, we only need to provide one argument: the name of the dataset. summary(LifeSat) ## LifSat age educ gender ## Min. : 13.00 Min. :64.00 Min. : 4.000 Min. :1.000 ## 1st Qu.: 44.25 1st Qu.:68.25 1st Qu.: 6.000 1st Qu.:1.000 ## Median : 58.00 Median :70.00 Median : 6.000 Median :1.000 ## Mean : 57.86 Mean :70.26 Mean : 6.541 Mean :1.449 ## 3rd Qu.: 69.75 3rd Qu.:72.00 3rd Qu.: 7.000 3rd Qu.:2.000 ## Max. :100.00 Max. :75.00 Max. :12.000 Max. :2.000 ## female ChildSup SpouSup SES ## Min. :0.000 Min. : 3.000 Min. : 2.000 Min. :1.000 ## 1st Qu.:0.000 1st Qu.: 6.000 1st Qu.: 5.000 1st Qu.:1.000 ## Median :1.000 Median : 7.000 Median : 6.000 Median :2.000 ## Mean :0.551 Mean : 6.857 Mean : 6.061 Mean :1.918 ## 3rd Qu.:1.000 3rd Qu.: 8.000 3rd Qu.: 8.000 3rd Qu.:2.000 ## Max. :1.000 Max. :10.000 Max. :10.000 Max. :3.000 Run the above code and check the output in the console. You should see descriptive statistics for each variable in the dataset. E.g., for LifSat, the minimum value is 13, the median is 58, and the mean is 57.86. "],["tutorialPlotting.html", "0.9 Plotting data", " 0.9 Plotting data It is almost always a good idea to visualize your data before you dive into a full statistical analysis. For example, you may like to know something about the nature of the relationship between two specific variables, the distribution of some set of values, etc. In this section, we will create three basic plots of our data. When we create these plots, the figures will appear in the plots tab in the lower right quadrant of the RStudio window. 0.9.1 R packages and ggplot2 We will use the ggplot2 package for plotting. This package is not part of the Base R installation. So, you must install ggplot2 yourself (you should have already done so in Section 0.2.3). You only need to install an R package once. R packages are just small software programs, so they must be installed just like any other piece of software. After installing, the package is accessible on your computer but not yet available for use in your current R session. You must first load the package. You need to load the package every time you want to use it in a new R session. After loading, the package contents (i.e., functions, help files, datasets) are initialized in memory and ready to use in your current R session. Loading a package is more-or-less equivalent to ‘opening’ a software program by clicking on its desktop icon. Run the following code to load the ggplot2 package. library(ggplot2) 0.9.2 Histogram We can use a histogram to visualize how the values of a continuous variable are distributed. ## Use the &#39;LifeSat&#39; data to create the plot ## Use the &#39;LifeSat$LifSat&#39; variable to define the X-axis ggplot(data = LifeSat, aes(x = LifSat)) + geom_histogram() # Create a histogram from the data/variable defined above 0.9.3 Boxplot A boxplot provides another useful visualization of a continuous variable’s distribution. We can also use boxplots to detect outliers. ## Same data/variable setup as above: ggplot(data = LifeSat, aes(x = LifSat)) + geom_boxplot() # Create a boxplot 0.9.4 Scatterplot A scatterplot provides a visual representation of the relationship between two variables. Since we are now plotting two variables, we need to define a y variable in addition to thex variable specified in the previous examples. ## Add the &#39;age&#39; variable on the Y-axis: ggplot(data = LifeSat, aes(x = LifSat, y = age)) + geom_point() # Create a scatterplot "],["manipulating-data.html", "0.10 Manipulating Data", " 0.10 Manipulating Data 0.10.1 Data types Recall the output of the str() function. One piece of information contained therein is the type of data stored in each column of our dataset. There a different abbreviations signifying different types of data. Abbreviation Type Description num Numeric All values are numbers (e.g., 1.02) chr Character All values are words log Logical Boolean flags: TRUE or FALSE factor Factor A special type of object with labels to represent the levels of a categorical variable 0.10.2 Factors Factors are a special type of data object that R uses to represent categorical variables. A factor is stored internally as a vector of integers where each group is represented by a different number. The groups also get descriptive labels. R knows that a factor is not numeric and will treat any factor as a nominal grouping variable for anlaysis. In the output from the str() function in 0.8.2, we see that the gender variable is stored as a numeric variable. You can confirm by running either of the following commands. is.numeric(LifeSat$gender) ## [1] TRUE class(LifeSat$gender) ## [1] &quot;numeric&quot; The gender varible is a binary grouping variable, so it should be stored as a factor. To convert gender to a factor, we can use the factor() function. ## Convert &#39;gender&#39; to a factor: LifeSat$gender &lt;- factor(LifeSat$gender) ## Check the results: is.numeric(LifeSat$gender) ## [1] FALSE class(LifeSat$gender) ## [1] &quot;factor&quot; str(LifeSat$gender) ## Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 2 2 2 2 1 2 1 2 1 1 ... We now see that gender has been converted to a factor with the levels “1” and “2”. We don’t have to settle for meaningless numeric labels, though. We can assign meaningful value labels by providing an appropriate input for the labels argument. ## Create a factor with meaningful labels: LifeSat$gender &lt;- factor(LifeSat$gender, labels = c(&quot;Female&quot;, &quot;Male&quot;)) ## Check the results: str(LifeSat$gender) ## Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 1 2 1 2 1 1 ... 0.10.3 Subsetting: Extracting one variable When working with data frames, we can extract a single variable (i.e., column) from the data using the dollar sign operator, $. As shown in the example below. # Extract the &#39;LifSat&#39; variable from &#39;LifeSat&#39;: LifeSat$LifSat ## [1] 13 18 19 24 24 24 30 33 33 33 33 33 35 35 37 37 41 41 41 ## [20] 43 43 43 44 44 44 45 47 48 48 48 50 51 51 52 53 53 53 53 ## [39] 54 55 55 56 56 56 57 58 58 58 58 58 58 59 59 60 61 61 63 ## [58] 63 63 65 66 67 67 67 67 68 68 68 69 69 69 69 69 70 70 70 ## [77] 71 72 74 74 76 77 77 78 78 79 79 79 81 81 82 83 85 86 87 ## [96] 91 99 100 In the above code, we ask R to extract the column named LifSat from the data frame named LifeSat. The result will be returned as another special type of object: a vector. 0.10.4 Subsetting: Extracting rows and columns We can also extract rectangular subsets of a data frame using the following convention: my_data_frame[row_numbers, column_numbers]. ## Extract the first four rows of the first two columns: LifeSat[1:4, 1:2] By leaving either rows or columns empty, we get all rows or columns: ## Extract all rows of first two columns: LifeSat[ , 1:2] ## Extract all columns of first two rows: LifeSat[1:2, ] We can refer to the columns by name, too: LifeSat[1:2, c(&quot;age&quot;, &quot;educ&quot;)] 0.10.5 Subsetting based on logical conditions We can also select rows or columns that satisfy logical conditions. In the following code, we select only the rows for which LifeSat$age is greater than 70. LifeSat[LifeSat$age &gt; 70, ] This approach works for any valid logical expression that will flag rows (or columns). Below, we select only the males and save the subset as a new object, LifeSat_male. LifeSat_male &lt;- LifeSat[LifeSat$gender == &quot;Male&quot;, ] str(LifeSat_male) ## &#39;data.frame&#39;: 44 obs. of 8 variables: ## $ LifSat : num 13 18 19 24 24 33 33 35 35 37 ... ## $ age : num 75 75 72 72 73 72 72 68 71 68 ... ## $ educ : num 6 5 5 6 6 5 12 6 4 7 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ female : num 0 0 0 0 0 0 0 0 0 0 ... ## $ ChildSup: num 4 6 6 6 8 7 5 6 7 4 ... ## $ SpouSup : num 2 5 5 4 6 6 6 6 7 4 ... ## $ SES : num 3 1 1 1 1 2 2 3 1 2 ... ## - attr(*, &quot;variable.labels&quot;)= Named chr [1:8] &quot;&quot; &quot;&quot; &quot;Years of education&quot; &quot;&quot; ... ## ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;LifSat&quot; &quot;age&quot; &quot;educ&quot; &quot;gender&quot; ... ## - attr(*, &quot;codepage&quot;)= int 1252 0.10.6 Changing cell values We can easily overwrite values in a dataset using the same type of subsetting operations shown above in combination with the assignment operator, &lt;-. The following code will extract the 5th entry in the LifSat variable. LifeSat[5, &quot;LifSat&quot;] ## [1] 24 Actually, it’s not really accurate to say that the above code “extracts” any value. The above command creates a new temporary object containing only the relevant value and prints the contents of that object to the console. All the subsetting examples above (other than the selection of all males) have done something similar. Rather than thinking about the [] or $ selection operators as ways of extracting pieces of a data object, it’s more appropriate to think about these operators as selecting, highlight, activating, nominating (or some other such concept) the referenced elements. Once the elements are so selected, we can also overwrite their original values. We only need to assign new values to the subset. To demonstrate, let’s overwrite the value selected above with 10. ## Overwrite the 5th &#39;LifSat&#39; value: LifeSat[5, &quot;LifSat&quot;] &lt;- 10 ## Check the result: LifeSat[5, &quot;LifSat&quot;] ## [1] 10 "],["getting-help.html", "0.11 Getting Help", " 0.11 Getting Help As you start to apply the techniques described in this guide, you will soon have questions that the guide does not answer. This section describes a few tips on how to get help answering these questions. Every function in R has documentation (i.e., a help file). To see this file in RStudio, select the name of the function in your script, and press F1, or run the command ? followed by the name of the function (e.g., ?aov). The second option works outside of RStudio, too. If you get stuck, start with Google. Typically, adding “R” to a search is enough to return relevant results (e.g., “exploratory factor analysis R”). Google is particularly useful for error messages. If you get an error message that you don’t understand, try googling it. Someone else has almost certainly been confused by the same message in the past, and there will be help somewhere on the web. If the error message isn’t in English, run Sys.setenv(LANGUAGE = \"en\") and re-run the code that produced the error (you’re more likely to find help for English error messages.) If Google doesn’t help, try stackoverflow. Before posting your question, spend some time searching the site for an existing answer (active contributors really hate it when you ask a question that has already been answered on the site). Including [R] in your search string restricts your search to questions and answers that use R. Lastly, if you find errors (or typos!) in this guide’s text or R syntax, feel free to contact me (Kyle Lang). "],["introduction.html", "1 Introduction", " 1 Introduction Homework before the lecture Complete the preparatory material: Read over the Course Information chapter Work through the Software Setup chapter Work through the R Tutorial chapter Watch the Lecture Recording for this week. Complete the Reading for this week, and answer the associated reading questions. Complete the Formative Assessment for this week. Lecture content We start with a brief introduction to the course, the course goals and rules, and the general idea of statistical modeling. We will introduce the type of models that we will consider in this course. We will shortly discuss several related concepts: Model simplicity/complexity Model fit Graphical representations of model parameters Interpretations of model parameters Homework before the practical Complete the At-Home Exercises. Practical content During the practical you will work on the In-Class Exercises. "],["lecture.html", "1.1 Lecture", " 1.1 Lecture Download slides The first lecture will be used to introduce the concept of fitting models to data and explain some important concepts and notation that will be used during this course. "],["reading.html", "1.2 Reading", " 1.2 Reading Reference Smaldino, P. E. (2017). Models are stupid, and we need more of them. Computational Social Psychology, 311–331. SKIP PAGES 322 - 327 Questions What are the differences between a “verbal model” and a “formal model”? As explained in the paragraph “A Brief Note on Statistical Models”, formal models are not the same as statistical models. Still, we can learn a lot from Smaldino’s approach. Write down three insights from this paper that you would like to apply to your statistical modeling during this course. Answers Q1: A verbal model can be non-specific about many things The level of analysis The definitions of its parts The relationships within the modeled system Verbal models can appear powerful and useful partly because they employ strategic ambiguity. Because verbal models are so vague, they can apparently explain many different phenomena, even contradictory ones (e.g., the Cubist chicken). A formal model, on the other hand, seeks to rigorously define its level of analysis, parts (variables), and relationships between these parts. Q2: You should define the level of analysis for your model You should clearly operationalize the parts (variables) and explicitly define the hypothesized relationships (causal, correlational, definitional) between these parts The act of formally defining our models is productive. We can test a well-defined model using data. We can discuss such a model among experts with relative consensus about what the model means, and we can improve it. These things are harder to do with vague verbal models. Every model imposes some violence upon reality. I.e., every model is wrong (but some are useful). Because a model is a simplification, by necessity, we leave out many factors. This simplification is a feature, not a bug. Models abstract away the uninteresting details (i.e., noise) of a phenomenon. Even models that are too simple to be interesting represent useful building-blocks to expand upon when creating more complex, interesting models. Data can be used to validate a model and to refine the model into a closer approximation of reality "],["formative-assessment.html", "1.3 Formative Assessment", " 1.3 Formative Assessment A formative assessment helps you judge your progress in the course and helps you address any blind spots in your understanding of the material. This assessment does not contribute to your course grade. If you get a question wrong, you will receive a hint on how to improve your understanding of the material. Ideally, you should complete this assessment after you have watched the lecture recording, but before the lecture meeting. This way, if you discover gaps in your understanding, we can discuss any topics that need more attention during the lecture meeting. Question 1: A model with more degrees of freedom will… fit the data better fit the data worse Question 2: The error term (\\(\\epsilon_i\\)) in regression equations reflects how much the observed scores of individuals differ from their predicted scores. FALSE TRUE Question 3: In OLS regression, a model is fit to the individual participant data. By contrast, regression in structural equation modeling fits a model to the observed covariance matrix. FALSE TRUE Question 4: In the equation \\(Y_i = a + bX_i + e_i\\), what are the ‘model parameters’? \\(a\\) and \\(bX_i\\) \\(a\\) and \\(b\\) \\(Y_i, X_i\\) and \\(e_i\\) \\(Y\\) and \\(X\\) Question 5: Multiple regression and ANCOVA are statistically equivalent. TRUE FALSE Question 6: What are degrees of freedom? The number of unique pieces of information minus number of parameters Number of participants minus number of parameters The number of unique pieces of information The number of parameters Question 7: What kind of model is depicted above? ANOVA with dummies Multiple regression A measurement model A path model Question 8: A psychologist administers a test intended to measure intelligence. Participants complete different puzzles and answer different questions. From a measurement theory point of view, what kind of variable is intelligence in this context? A latent variable A dependent variable A measurement variable An observed variable Question 9: A model with more degrees of freedom is… more simple more complex "],["at-home-exercises.html", "1.4 At-Home Exercises", " 1.4 At-Home Exercises Load the LifeSat.sav data. library(foreign) LifeSat &lt;- read.spss(&quot;LifeSat.sav&quot;, to.data.frame = TRUE) 1.4.1 Make a table of descriptive statistics for the variables: LifSat, educ, ChildSup, SpouSup, and age. What is the average age in the sample? What is the ange (youngest and oldest child)? Hint: Use the tidySEM::descriptives() function.` Click for explanation The package tidySEM contains the descriptives() function for computing descriptive statistics. The describe() function in the psych package is a good alternative. library(tidySEM) descriptives(LifeSat[ , c(&quot;LifSat&quot;, &quot;educ&quot;, &quot;ChildSup&quot;, &quot;SpouSup&quot;, &quot;age&quot;)]) 1.4.2 Run a simple linear regression with LifSat as the dependent variable and educ as the independent variable. Hints: The lm() function (short for linear model) does linear regression. The summary() function provides relevant summary statistics for the model. It can be helpful to store the results of your analysis in an object. Click for explanation results &lt;- lm(LifSat ~ educ, data = LifeSat) summary(results) ## ## Call: ## lm(formula = LifSat ~ educ, data = LifeSat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -44.158 -11.678 2.212 12.541 43.212 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.418 8.003 4.301 4.09e-05 *** ## educ 3.562 1.192 2.988 0.00356 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.93 on 96 degrees of freedom ## Multiple R-squared: 0.08511, Adjusted R-squared: 0.07558 ## F-statistic: 8.931 on 1 and 96 DF, p-value: 0.00356 1.4.3 Repeat the analysis from 1.4.2 with age as the independent variable. Click for explanation results &lt;- lm(LifSat ~ age, data = LifeSat) summary(results) ## ## Call: ## lm(formula = LifSat ~ age, data = LifeSat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.230 -14.025 3.321 13.745 40.770 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 199.6492 53.1261 3.758 0.000294 *** ## age -2.0203 0.7557 -2.673 0.008829 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.08 on 96 degrees of freedom ## Multiple R-squared: 0.06928, Adjusted R-squared: 0.05959 ## F-statistic: 7.146 on 1 and 96 DF, p-value: 0.008829 1.4.4 Repeat the analysis from 1.4.2 and 1.4.3 with ChildSup as the independent variable. Click for explanation results &lt;- lm(LifSat ~ ChildSup, data = LifeSat) summary(results) ## ## Call: ## lm(formula = LifSat ~ ChildSup, data = LifeSat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -45.131 -11.875 0.862 12.595 44.869 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.052 8.487 4.366 3.2e-05 *** ## ChildSup 3.013 1.208 2.494 0.0144 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.17 on 96 degrees of freedom ## Multiple R-squared: 0.06083, Adjusted R-squared: 0.05105 ## F-statistic: 6.218 on 1 and 96 DF, p-value: 0.01436 1.4.5 Run a multiple linear regression with LifSat as the dependent variable and educ, age, and ChildSup as the independent variables. Hint: You can use the + sign to add multiple variables to the RHS of your model formula. Click for explanation results &lt;- lm(LifSat ~ educ + age + ChildSup, data = LifeSat) summary(results) ## ## Call: ## lm(formula = LifSat ~ educ + age + ChildSup, data = LifeSat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.525 -12.472 2.601 11.235 42.108 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 132.5942 54.2325 2.445 0.0164 * ## educ 2.9123 1.1640 2.502 0.0141 * ## age -1.5766 0.7316 -2.155 0.0337 * ## ChildSup 2.4554 1.1564 2.123 0.0364 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 17.23 on 94 degrees of freedom ## Multiple R-squared: 0.1732, Adjusted R-squared: 0.1468 ## F-statistic: 6.565 on 3 and 94 DF, p-value: 0.0004452 1.4.6 Compare the results from 1.4.5 with those from 1.4.2, 1.4.3, and 1.4.4. What do you notice when you compare the estimated slopes for each of the three predictors in the multiple regression model with the corresponding estimates from the simple regression models? "],["in-class-exercises.html", "1.5 In-Class Exercises", " 1.5 In-Class Exercises During this practical, you will work through some exercises about ANOVA, ANCOVA, and regression. Note that ANOVA and ANCOVA are special cases of regression. Or, more accurately, ANOVA, ANCOVA, and regression are all different flavors of the general linear model. If you need to refresh your knowledge on ANOVA, ANCOVA, or regression consider the resources listed in the Background knowledge section. 1.5.1 Part 1: Data Exploration 1.5.1.1 Open the file Sesam.sav: ## Load the &#39;foreign&#39; library for reading SPSS files: library(foreign) ## Load the &#39;Sesam.sav&#39; data into an object called &#39;sesam&#39;: sesam &lt;- read.spss(&quot;Sesam.sav&quot;, to.data.frame = TRUE, use.value.labels = FALSE) This file is part of a larger dataset that evaluates the impact of the first year of the Sesame Street television series. Sesame Street is mainly concerned with teaching preschool related skills to children in the 3–5 year age range. The following variables will be used in this exercise: age: measured in months prelet: knowledge of letters before watching Sesame Street (range 0–58) prenumb: knowledge of numbers before watching Sesame Street (range 0–54) prerelat: knowledge of relations before watching Sesame Street (range 0–17) peabody: vocabulary maturity before watching Sesame Street (range 20–120) postnumb: knowledge of numbers after a year of Sesame Street (range 0–54) Note: Unless otherwise noted, the following questions refer to the sesam data and the above variables. 1.5.1.2 What is the measurement level of each variable? Hint: The output of the str() function should be helpful here. Click for explanation ## Examine the data structure: str(sesam) ## &#39;data.frame&#39;: 240 obs. of 8 variables: ## $ id : num 1 2 3 4 5 6 7 8 9 10 ... ## $ age : num 66 67 56 49 69 54 47 51 69 53 ... ## $ prelet : num 23 26 14 11 47 26 12 48 44 38 ... ## $ prenumb : num 40 39 9 14 51 33 13 52 42 31 ... ## $ prerelat: num 14 16 9 9 17 14 11 15 15 10 ... ## $ peabody : num 62 80 32 27 71 32 28 38 49 32 ... ## $ postnumb: num 44 39 40 19 54 39 44 51 48 52 ... ## $ gain : num 4 0 31 5 3 6 31 -1 6 21 ... ## - attr(*, &quot;codepage&quot;)= int 65001 All variables are numeric. 1.5.1.3 What is the average age in the sample? What is the age range (youngest and oldest child)? Hint: Use tidySEM::descriptives() Click for explanation As in the take home exercises, you can use the descriptives() function from the tidySEM package to describe the data: library(tidySEM) descriptives(sesam) 1.5.1.4 What is the average gain in knowledge of numbers? What is the standard deviation of this gain? Hints: You will need to compute the gain and save the change score as a new object. You can then use the base-R functions mean() and sd() to do the calculations. Click for explanation Create a new variable that represents the difference between pre- and post-test scores on knowledge of numbers: sesam$ndif &lt;- sesam$postnumb - sesam$prenumb Compute the mean and SD of the change score: mean(sesam$ndif) ## [1] 9.158333 sd(sesam$ndif) ## [1] 9.682401 1.5.1.5 Create an appropriate visualization of the gain scores you computed in 1.5.1.4. Justify your choice of visualization. Hint: Some applicable visualizations are explained in 0.9. Click for explanation library(ggplot2) ## Create an empty baseline plot object: p &lt;- ggplot(sesam, aes(x = ndif)) ## Add some appropriate geoms: p + geom_histogram() p + geom_density() p + geom_boxplot() 1.5.1.6 Create a visualization that provides information about the bivariate relationship between two of the variables. Justify your choice of visualization. Hint: Again, Section 0.9 may provide some useful insights. Click for explanation ## Create a scatterplot of the pre- and post-test number knowledge ggplot(sesam, aes(x = prenumb, y = postnumb)) + geom_point() 1.5.2 Part 2: Regression Analysis 1.5.2.1 Are there significant, bivariate associations between postnumb and the following variables? age prelet prenumb prerelat peabody Use Pearson correlations to answer this question. You do not need to check the assumptions here (though you would in real life). Hint: The base-R cor.test() function and the corr.test() function from the psych package will both conduct hypothesis tests for a correlation coefficients (the base-R cor() function only computes the coefficients). Click for explanation library(psych) ## Test the correlations using psych::corr.test(): corr.test( sesam[ , c(&quot;postnumb&quot;, &quot;age&quot;, &quot;prelet&quot;, &quot;prenumb&quot;, &quot;prerelat&quot;, &quot;peabody&quot;)] ) ## Call:corr.test(x = sesam[, c(&quot;postnumb&quot;, &quot;age&quot;, &quot;prelet&quot;, &quot;prenumb&quot;, ## &quot;prerelat&quot;, &quot;peabody&quot;)]) ## Correlation matrix ## postnumb age prelet prenumb prerelat peabody ## postnumb 1.00 0.34 0.50 0.68 0.54 0.52 ## age 0.34 1.00 0.33 0.43 0.44 0.29 ## prelet 0.50 0.33 1.00 0.72 0.47 0.40 ## prenumb 0.68 0.43 0.72 1.00 0.72 0.61 ## prerelat 0.54 0.44 0.47 0.72 1.00 0.56 ## peabody 0.52 0.29 0.40 0.61 0.56 1.00 ## Sample Size ## [1] 240 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## postnumb age prelet prenumb prerelat peabody ## postnumb 0 0 0 0 0 0 ## age 0 0 0 0 0 0 ## prelet 0 0 0 0 0 0 ## prenumb 0 0 0 0 0 0 ## prerelat 0 0 0 0 0 0 ## peabody 0 0 0 0 0 0 ## ## To see confidence intervals of the correlations, print with the short=FALSE option ## OR ## ## Test the correlations using multiple cor.test() calls: cor.test(sesam$postnumb, sesam$age) ## ## Pearson&#39;s product-moment correlation ## ## data: sesam$postnumb and sesam$age ## t = 5.5972, df = 238, p-value = 5.979e-08 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.2241066 0.4483253 ## sample estimates: ## cor ## 0.3410578 cor.test(sesam$postnumb, sesam$prelet) ## ## Pearson&#39;s product-moment correlation ## ## data: sesam$postnumb and sesam$prelet ## t = 8.9986, df = 238, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4029239 0.5926632 ## sample estimates: ## cor ## 0.5038464 cor.test(sesam$postnumb, sesam$prenumb) ## ## Pearson&#39;s product-moment correlation ## ## data: sesam$postnumb and sesam$prenumb ## t = 14.133, df = 238, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6002172 0.7389277 ## sample estimates: ## cor ## 0.6755051 cor.test(sesam$postnumb, sesam$prerelat) ## ## Pearson&#39;s product-moment correlation ## ## data: sesam$postnumb and sesam$prerelat ## t = 9.9857, df = 238, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4475469 0.6268773 ## sample estimates: ## cor ## 0.5433818 cor.test(sesam$postnumb, sesam$peabody) ## ## Pearson&#39;s product-moment correlation ## ## data: sesam$postnumb and sesam$peabody ## t = 9.395, df = 238, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4212427 0.6067923 ## sample estimates: ## cor ## 0.520128 1.5.2.2 Do age and prenumb explain a significant proportion of the variance in postnumb? What statistic did you use to justify your conclusion? Interpret the model fit. Hints: The lm() function (short for linear model) estimates linear regression models. The summary() function provides relevant summary statistics for the model. Click for explanation results &lt;- lm(postnumb ~ age + prenumb, data = sesam) summary(results) ## ## Call: ## lm(formula = postnumb ~ age + prenumb, data = sesam) ## ## Residuals: ## Min 1Q Median 3Q Max ## -38.130 -6.456 -0.456 5.435 22.568 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.4242 5.1854 1.432 0.154 ## age 0.1225 0.1084 1.131 0.259 ## prenumb 0.7809 0.0637 12.259 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.486 on 237 degrees of freedom ## Multiple R-squared: 0.4592, Adjusted R-squared: 0.4547 ## F-statistic: 100.6 on 2 and 237 DF, p-value: &lt; 2.2e-16 Yes, age and prenumb explain a significant amount of variability in postnumb (\\(R^2 = 0.459\\), \\(F[2, 237] = 100.629\\), \\(p &lt; 0.001\\)). We use the F statistic for the overall test of model fit to support this conclusion. The variables age and prenumb together explain 45.9% of the variability in postnumb. 1.5.2.3 Write the null and alternative hypotheses for tested in 1.5.2.2. Click for explanation Since we are testing for explained variance, our hypotheses concern the \\(R^2\\). \\[ \\begin{align*} H_0: R^2 = 0\\\\ H_1: R^2 &gt; 0 \\end{align*} \\] Note that this is a directional hypotheses because the \\(R^2\\) cannot be negative. 1.5.2.4 Consider the path model below. How many regression coefficients are estimated in this model? How many variances are estimated? How many covariances are estimated? How many degrees of freedom does this model have? Hint: As you learned in Lecture 1, \\(df = N_{obs} – N_{par}\\). 1.5.2.5 Consider a multiple regression analysis with three continuous independent variables: scores on tests of language, history, and logic, and one continuous dependent variable: score on a math test. We want to know if scores on the language, history, and logic tests can predict the math test score. Sketch a path model that you could use to answer this question How many regression parameters are there? How many variances could you estimate? How many covariances could you estimate? How many degrees of freedom does this model have? 1.5.3 Part 3: ANOVA 1.5.3.1 Load the Drivers.sav data. # Read the data into a data frame named &#39;drivers&#39;: drivers &lt;- read.spss(&quot;Drivers.sav&quot;, to.data.frame = TRUE) In this section, we will use ANOVA to evaluate the following research question: Does talking on the phone interfere with people’s driving skills? These data come from an experiment. The condition variable represents the three experimental conditions: Hand-held phone Hands-free phone Control (no phone) We will use condition as the IV in our ANOVA models. The DV, RT, represents the participant’s reaction time (in milliseconds) during a driving simulation. ANOVA vs. Linear Regression As you may know, the mathematical model underlying ANOVA is just a linear regression model with nominal IVs. So, in terms of the underlying statistical models, there is no difference between ANOVA and regression; the differences lie in the focus of the analysis. ANOVA is really a type of statistical test wherein we are testing hypotheses about the effects of some set of nominal grouping factors on some continuous outcome. When doing an ANOVA, we usually don’t interact directly with the parameter estimates from the underlying model. Regression is a type of statistical model (i.e., a way to represent a univariate distribution with a conditional mean and fixed variance). When we do a regression analysis, we primarily focus on the estimated parameters of the underling linear model. When doing ANOVA in R, we estimate the model exactly as we would for linear regression; we simply summarize the results differently. 1.5.3.2 Perform the ANOVA to test the above research question. Hint: After estimating the model with lm(), you can use the anova() function to compute the sums-of-squares and significance tests for each factor in your model. Click for explanation ## Estimate the underlying model: results &lt;- lm(RT ~ condition, data = drivers) ## Summarize the model as a regression analysis: summary(results) ## ## Call: ## lm(formula = RT ~ condition, data = drivers) ## ## Residuals: ## Min 1Q Median 3Q Max ## -317.50 -71.25 2.98 89.55 243.45 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 654.50 29.08 22.506 &lt;2e-16 *** ## conditionhands-free -36.95 41.13 -0.898 0.3727 ## conditioncontrol -100.75 41.13 -2.450 0.0174 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 130.1 on 57 degrees of freedom ## Multiple R-squared: 0.09729, Adjusted R-squared: 0.06562 ## F-statistic: 3.072 on 2 and 57 DF, p-value: 0.05408 ## Summarize the model as an ANOVA: anova(results) Of course the results of any analysis are only valid if the assumptions of the analysis/model are satisfied. In particular, we should probably check at least three conditions: There are no overly influential cases The residual variance is homogonous across groups The residuals are normally distributed 1.5.3.3 Check for influential cases. Hint: You can use the cooks.distance() function to compute Cook’s Distance statistic for each observation. Observations with Cook’s D values substantially larger than, and qualitatively distinct from, the rest of the data may be overly influential. You can evaluate the relative sizes of the distances by making an index plot of the estimated distance statistics. Click for explanation ## Compute the Cook&#39;s distances: d &lt;- cooks.distance(results) ## Plot the distances: plot(d) In the figure above, we’re looking for any distances that clearly “stand out from the crowd”. None of the distances in the above figure look notable. We do not see evidence of influential observations. 1.5.3.4 Check the normality of the residuals. Hints: One of the best ways to check the normality of residuals is with a Normal QQplot. We can easily create a Normal QQ-Plot of the residuals by plotting the results object from 1.5.3.2. Click for explanation plot(results, 2) In the QQ-Plot created above, we want to see all of the points follow the diagonal, dashed line. Perfectly normal residuals will fall exactly along this line Deviations away from the line indicate devations from normality. The residuals in this figure look quite good. We only see very minor deviations from the idealized line. The residuals appear to be more-or-less normally distributed. 1.5.3.5 Check the homogeneity of the residual variances. Hints: A scale-location plot is one of the best ways to check the homogeneity of variances assumption. Plot an estimate of the residual variance against the predicted values Any trend indicates differences in residual variance between groups Plotting the results object from 1.5.3.2 will also produce a scale-location plot. Click for explanation plot(results, 3) The red line in this figure is a loess line which represents the trend of the plotted data. If this loess line is flat, there is no evidence of differences in the residual variances between groups. Trends in this line indicate violations of the homogeneity assumption. In this case, the line is pretty much flat and we have little-to-no evidence of heterogeneous residual variances. 1.5.3.6 Summarize your conclusions regarding the assumption. Are the assumptions satisfied? Can we trust the model results? Click for explanation There are no observations that stand out as particularly influential. Furthermore, we have no evidence of heterogeneous residual variances or substantial violations of normality for the residuals. Hence, the assumptions appear to be satisfied, and we can trust the conclusions of our analysis. 1.5.3.7 Use your results to answer the research question. Click for explanation anova(results) The effect of condition on RT was nonsignificant (\\(F[2, 57] = 3.07\\), \\(p = 0.054\\)). Therefore, based on these results, we do not have evidence for an effect of mobile phone usage on driving performance. 1.5.4 Part 4: ANCOVA We will now conduct an ANCOVA to assess the following research question. Are there differences in reaction times between the phone conditions after controlling for age? As with ANOVA, the statistical model underlying an ANCOVA is simply a linear regression model. The model for an ANCOVA, however, includes at least one continuous covariate in addition to the categorical IVs. Consequently, when we conduct ANCOVA in R, we again use the lm() function to estimate the model. 1.5.4.1 Estimate the ANCOVA model needed to test the above research question. Click for explanation ## Estimate the model: results &lt;- lm(RT ~ condition + age, data = drivers) ## Conduct the statistical tests: anova(results) ANCOVA and Interactions As you can see above, when we run an ANCOVA, we’re just estimating a multiple linear regression model. We call the analysis ANCOVA because we are interested in a specific hypothesis. Our substantive interest lies in the categorical IVs. The continuous covariates are uninteresting, nuisance variables that we are controlling for to get better estimates of the interesting treatment effects. One implication of this hypothesis is the absence of any interaction between the covariates and grouping factors. If the covariate moderates the treatment effect, the covariate has a direct impact on the substantively interesting group differences. Such a covariate is not a covariate; it’s a substantively integral feature of the model. If we want to report our analysis as an ANCOVA, we need to show that the covariate effects are equivalent in each group. In other words, there is no interaction between the grouping factors and the covariates. 1.5.4.2 Test for an interaction between age and condition in the model from 1.5.4.1. What is your conclusion? Can we report our analysis as an ANCOVA? Hint: You can include an interaction term in an R formulas ‘multiplying’ the two variable names using the * operator. Click for explanation ## Add the interaction between &#39;age&#39; and &#39;condition&#39; to the model: results_int &lt;- lm(RT ~ condition * age, data = drivers) ## Conduct the hypothesis tests. anova(results_int) The interaction between condition and age is not significant (\\(F[2, 54] = 1.07\\), \\(p = 0.349\\)). Therefore, the covariate effects should be more-or-less equivalent in each group, and we can report our analysis as an ANCOVA. 1.5.4.3 Answer the research from above question. If you did find a significant, partial effect of condition, do a post hoc comparison of pairwise mean differences to see which groups showed significantly different reaction times, after controlling for age. Use Tukey’s HSD correction for all pairwise comparisons to control the Type I error rate. Hints: You can conduct the appropriate post hoc test with the TukeyHSD() function. The TukeyHSD() function only works on models estimated using the aov() function (which is the same as lm() but summarizes the results in ANOVA style). To satisfy TukeyHSD(), you can either rerun your model with aov() or convert your results object to the correct format via aov(results). Click for explanation ## Check the hypothesis tests for the ANCOVA from above: anova(results) ## Run the post hoc comparisons: TukeyHSD(aov(results)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = results) ## ## $condition ## diff lwr upr p adj ## hands-free-hand-held -36.95 -118.5708 44.67082 0.5242511 ## control-hand-held -100.75 -182.3708 -19.12918 0.0119407 ## control-hands-free -63.80 -145.4208 17.82082 0.1533777 After controlling for age, phone usage significantly affects driving performance (\\(F[2, 56] = 4.52\\), \\(p = 0.015\\)). Specifically, the hand-held condition has a significant higher reaction time than the control condition. 1.5.5 Part 5: Back to Regression As we saw above, we can use ANOVA or ANCOVA to test hypotheses about differences between groups in some continuous outcome. AN(C)OVA is a type of statistical test, though, not a type of model. The statistical model underlying an AN(C)OVA is just a linear regression model. Since this class is about statistical modeling, it won’t do us much good to keep thinking in terms of statistical tests; we’re better off approaching these problems from a modeling perspective. We can just as easily make the inferences from above by working directly within a regression modeling framework. In this section, we will explore linear regression models with categorical predictors. 1.5.5.1 Load the Sesam2.sav data. # Read the data into an object called &#39;sesam2&#39;: sesam2 &lt;- read.spss(&quot;Sesam2.sav&quot;, to.data.frame = TRUE) 1.5.5.2 VIEWCAT is a nominal grouping variable, but it is represented as a numeric variable in the sesam2 data. Convert VIEWCAT into a factor. Make sure that VIEWCAT = 1 is the reference group. Hints: You can identify the reference group with the levels() or contrasts() functions. The reference group is the group labelled with the first level printed by levels(). When you run contrasts(), you will see a pattern matrix that defines a certain dummy coding scheme. The reference group is the group that has zeros in each column of this matrix. If you need to change the reference group, you can use the relevel() function. Click for explanation ## Convert &#39;VIEWCAT&#39; to a factor: sesam2$VIEWCAT &lt;- factor(sesam2$VIEWCAT) ## Check the reference group: levels(sesam2$VIEWCAT) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; contrasts(sesam2$VIEWCAT) ## 2 3 4 ## 1 0 0 0 ## 2 1 0 0 ## 3 0 1 0 ## 4 0 0 1 1.5.5.3 Estimate a multiple regression model wherein VIEWCAT predicts POSTNUMB. Summarize the model. Interpret the estimates. Click for explanation results &lt;- lm(POSTNUMB ~ VIEWCAT, data = sesam2) summary(results) ## ## Call: ## lm(formula = POSTNUMB ~ VIEWCAT, data = sesam2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.474 -7.942 0.240 8.526 25.240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 18.760 2.316 8.102 8.95e-14 *** ## VIEWCAT2 9.331 2.900 3.218 0.00154 ** ## VIEWCAT3 14.714 2.777 5.298 3.49e-07 *** ## VIEWCAT4 18.032 2.809 6.419 1.24e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.58 on 175 degrees of freedom ## Multiple R-squared: 0.2102, Adjusted R-squared: 0.1967 ## F-statistic: 15.53 on 3 and 175 DF, p-value: 5.337e-09 1.5.5.4 Use ggplot() to make a scatterplot with AGE on the x-axis and POSTNUMB on the y-axis. Color the points according to the their VIEWCAT level. Save the plot object to a variable in your environment. Hint: You can map color to the levels of a variable on your dataset by assigning the variable names to the color argument of the aes() function in ggplot(). Click for explanation library(ggplot2) ## Add aes(..., color = VIEWCAT) to get different colors for each group: p &lt;- ggplot(sesam2, aes(x = AGE, y = POSTNUMB, color = VIEWCAT)) + geom_point() # Add points for scatterplot ## Print the plot stored as &#39;p&#39;: p We assigned the global color aesthetic to the VIEWCAT variable, so the points are colored based on their group. 1.5.5.5 Add linear regression lines for each group to the above scatterplot. Hints: You can add regression lines with ggplot2::geom_smooth() To get linear regression lines, set the argument method = \"lm\" To omit error envelopes, set the argument se = FALSE Click for explanation ## Add OLS best-fit lines: p + geom_smooth(method = &quot;lm&quot;, se = FALSE) The global color aesthetic assignment from above carries through to any additional plot elements that we add, including the regression lines. So, we also get a separate regression line for each VIEWCAT group. 1.5.5.6 How would you interpret the pattern of regression lines above? Click for explanation All the lines show a positive slope, so post-test number recognition appears to increase along with increasing age. The lines are not parallel, though. So, VIEWCAT may be moderating the effect of AGE on POSTNUMB. Moderated Regression Based on the figure we just created, we may want to test for moderation in our regression model. To do so, we need to add an interaction between AGE and VIEWCAT. The VIEWCAT factor is represented by 3 in our model, though. So, when we interact AGE and VIEWCAT, we will create 3 interaction terms. To test the overall moderating influence of VIEWCAT, we need to conduct a multiparameter hypothesis test of all 3 interaction terms. One way that we can go about implementing such a test is through a hierarchical regression analysis entailing three steps: Estimate the additive model wherein we regress POSTNUMB onto AGE and VIEWCAT without any interaction. Estimate the moderated model by adding the interaction between AGE and VIEWCAT into the additive model. Conduct a \\(\\Delta R^2\\) test to compare the fit of the two models. 1.5.5.7 Conduct the hierarchical regression analysis described above. Does VIEWCAT significantly moderate the effect of AGE on POSTNUMB? Provide statistical justification for your conclusion. Click for explanation ## Estimate the additive model a view the results: results_add &lt;- lm(POSTNUMB ~ VIEWCAT + AGE, data = sesam2) summary(results_add) ## ## Call: ## lm(formula = POSTNUMB ~ VIEWCAT + AGE, data = sesam2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.680 -8.003 -0.070 8.464 22.635 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -10.1056 6.5091 -1.553 0.12235 ## VIEWCAT2 9.1453 2.7390 3.339 0.00103 ** ## VIEWCAT3 13.8602 2.6294 5.271 3.98e-07 *** ## VIEWCAT4 16.9215 2.6636 6.353 1.79e-09 *** ## AGE 0.5750 0.1221 4.708 5.08e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.94 on 174 degrees of freedom ## Multiple R-squared: 0.2995, Adjusted R-squared: 0.2834 ## F-statistic: 18.6 on 4 and 174 DF, p-value: 9.642e-13 ## Estimate the moderated model and view the results: results_mod &lt;- lm(POSTNUMB ~ VIEWCAT * AGE, data = sesam2) summary(results_mod) ## ## Call: ## lm(formula = POSTNUMB ~ VIEWCAT * AGE, data = sesam2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.8371 -8.2387 0.6158 8.7988 22.5611 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -18.7211 15.5883 -1.201 0.2314 ## VIEWCAT2 9.9741 20.6227 0.484 0.6293 ## VIEWCAT3 23.5825 19.3591 1.218 0.2248 ## VIEWCAT4 34.3969 19.3600 1.777 0.0774 . ## AGE 0.7466 0.3074 2.429 0.0162 * ## VIEWCAT2:AGE -0.0175 0.4060 -0.043 0.9657 ## VIEWCAT3:AGE -0.1930 0.3782 -0.510 0.6104 ## VIEWCAT4:AGE -0.3416 0.3770 -0.906 0.3663 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.99 on 171 degrees of freedom ## Multiple R-squared: 0.3046, Adjusted R-squared: 0.2762 ## F-statistic: 10.7 on 7 and 171 DF, p-value: 3.79e-11 ## Test for moderation: anova(results_add, results_mod) VIEWCAT does not significantly moderate the effect of AGE on POSTNUMB (\\(F[3, 171] = 0.422\\), \\(p = 0.738\\)). 1.5.5.8 Write the regression equations for the additive and moderated models from 1.5.5.7. Click for explanation Additive Model \\[ \\begin{align*} Y_{postnumb} = \\beta_0 + \\beta_1 X_{view2} + \\beta_2 X_{view3} + \\beta_3 X_{view4} + \\beta_4 X_{age} + \\varepsilon \\end{align*} \\] Moderated Model \\[ \\begin{align*} Y_{postnumb} = \\beta_0 + &amp;\\beta_1 X_{view2} + \\beta_2 X_{view3} + \\beta_3 X_{view4} + \\beta_4 X_{age} +\\\\ &amp;\\beta_5 X_{view2} X_{age} + \\beta_6 X_{view3} X_{age} + \\beta_7 X_{view4} X_{age} + \\varepsilon \\end{align*} \\] 1.5.5.9 Write the null and alternative hypotheses for the test of moderation you conducted in 1.5.5.7. Click for explanation \\[ \\begin{align*} H_0: \\Delta R^2 = 0\\\\ H_1: \\Delta R^2 &gt; 0 \\end{align*} \\] 1.5.5.10 Write the regression equation for each of the four VIEWCAT groups. Click for explanation VIEWCAT 1: \\[ Y_{postnumb} = \\beta_0 + \\beta_4 X_{age} + \\varepsilon \\] VIEWCAT 2: \\[ Y_{postnumb} = \\beta_0 + \\beta_1 X_{view2} + \\beta_4 X_{age} + \\beta_5 X_{view2} X_{age} + \\varepsilon \\] VIEWCAT 3: \\[ Y_{postnumb} = \\beta_0 + \\beta_2 X_{view3} + \\beta_4 X_{age} + \\beta_6 X_{view3} X_{age} + \\varepsilon \\] VIEWCAT 4: \\[ Y_{postnumb} = \\beta_0 + \\beta_3 X_{view4} + \\beta_4 X_{age} + \\beta_7 X_{view4} X_{age} + \\varepsilon \\] 1.5.5.11 Run the anova() function on only the moderated regression model from 1.5.5.7. Compare the result to the anova() output from 1.5.5.7. What do you notice when comparing the two anova() outputs? Does this comparison offer any insight into the relation between regression and AN(C)OVA? Click for explanation anova(results_mod) anova(results_add, results_mod) When we run the anova() function on only the moderated regression model, we get ANCOVA-style tests like those we used in 1.5.4.2. The marginal test for the interaction between AGE and VIEWCAT in this ANCOVA-style result is identical to the model comparison test from 1.5.5.7. This comparison shows that the significance tests for marginal effects in an AN(C)OVA model can be equivalent to certain model comparison tests in a regression model. Indeed, although you cannot know this from only the examples explored here, any significance test in an AN(C)OVA can be reformulated as a comparison between two appropriately nested linear regression models. End of In-Class Exercises 1 "],["efa.html", "2 EFA", " 2 EFA Homework before the lecture Watch the Lecture Recording for this week. Complete the Reading for this week, and answer the associated reading questions. Complete the Formative Assessment for this week. Lecture content This lecture constitutes a general introduction to latent variables and scaling procedures. We will discuss several different aspects of exploratory factor analysis (EFA). Most notably: The differences between Principal Component Analyses (PCA) and Factor Analysis Model estimation and factor extraction methods Factor rotations You will have to make decisions regarding each of these aspects when conducting a factor analysis. We will also discuss reliability and factor scores as means of evaluating the properties of a scale. Homework before the practical Complete the At-Home Exercises. Practical content During the practical you will work on the In-Class Exercises. "],["lecture-1.html", "2.1 Lecture", " 2.1 Lecture Download slides How do you know if you have measured the putative hypothetical construct that you intend to measure? The methods introduced in this lecture (namely, latent variables, factor analysis, and reliability analysis) can shed empirical light on this issue. In the social and behavioral sciences we’re often forced to measure key concepts indirectly. For example, we have no way of directly quantifying a person’s current level of depression, or their innate motivation, or their risk-aversion, or any of the other myriad psychological features that comprise the human mental state. In truth, we cannot really measure these hypothetical constructs at all, we must estimate latent representations thereof (though, psychometricians still use the language of physical measurement to describe this process). Furthermore, we can rarely estimate an adequate representation with only a single observed variable (e.g., question on a survey, score on a test, reading from a sensor). We generally need several observed variables to reliably represent a single hypothetical construct. For example, we cannot accurately determine someone’s IQ or socio-economic status based on their response to a single question; we need several questions that each tap into slightly different aspects of IQ or SES. Given multiple items measuring the same construct, we can use the methods discussed in this lecture (i.e., factor analysis and reliability analysis) to evaluate the quality of our measurement (i.e., how well we have estimated the underlying hypothetical construct). If we do well enough in this estimation task, we will be able to combine these estimated latent variables with the path analysis methods discussed in two weeks to produce the full structural equation models that we will cover at the end of this course. "],["reading-1.html", "2.2 Reading", " 2.2 Reading This week, you will read two papers. Reference 1 Preacher, K. J., &amp; MacCullum, R. C. (2003). Repairing Tom Swift’s electric factor analysis machine, Understanding Statistics 2(1) 13–43. Questions 1 What is a latent variable? Give an example of a latent variable. What is factor analysis, and what can you investigate using this method? In the introduction, Preacher and Maccallum describe a “little jiffy” method of doing factor analysis. Briefly describe this little jiffy—or bad practice—method. Briefly explain the key differences between Principal Component Analyses (PCA) and Exploratory Factor Analyses (EFA). What is the purpose of factor rotation? Reference 2 Kestilä, E. (2006). Is there demand for radical right populism in the Finnish electorate? Scandinavian Political Studies 29(3), 169–191. Questions 2 What is the research question that the author tries to answer? Briefly describe the characteristics of the Radical Right Parties (RRP) in Europe. What are the two main explanations of support for RRP upon which this paper focuses? Does the empirical part of the paper reflect the theoretical framework well? Why or why not? According to the author, is Finland very different from other European countries on the main dependent variables? What is the author’s conclusion (i.e., how does the author answer the research question)? "],["formative-assessment-1.html", "2.3 Formative Assessment", " 2.3 Formative Assessment A formative assessment helps you judge your progress in the course and helps you address any blind spots in your understanding of the material. This assessment does not contribute to your course grade. If you get a question wrong, you will receive a hint on how to improve your understanding of the material. Ideally, you should complete this assessment after you have watched the lecture recording, but before the lecture meeting. This way, if you discover gaps in your understanding, we can discuss any topics that need more attention during the lecture meeting. Question 1: What is the maximum number of latent variables when conducting Exploratory Factor Analysis with 5 items? No limit; you can keep adding latent variables, but they will explain ever decreasing amounts of variance. Only one; you need at least 3 indicators per latent variable. Two; you need 3 indicators for one latent variable, but the model is still identified due to the covariance between latent variables. Five; the same as the number of indicators. Question 2: Promax rotation in factor analysis is a specific form of orthogonal rotation. FALSE TRUE Question 3: Which of these is not a valid method for choosing the correct number of factors? Kaiser’s criterion Ability to predict a criterion scale. Theory Parallel analysis Scree plot Question 4: When conducting an exploratory factor analysis, cross-loadings are assumed to be zero. FALSE TRUE Question 5: What kind of model is displayed above? Confirmatory Factor Analysis Principal Components Analysis An Item Response Theory Analysis Exploratory Factor Analysis Question 6: Order these methods for determining the number of factors from least subjective to most subjective: 1) Kaiser’s criterion, 2) Scree plot, 3) Parallel analysis. 1, 2, 3 3, 1, 2 3, 2, 1 2, 1, 3 Question 7: Factor analysis is a statistical technique aimed at data reduction. TRUE FALSE Question 8: With PCA, you must indicate the number of factors to extract a-priori. FALSE TRUE Question 9: EFA models the covariances among items; PCA describes the total variance of items. TRUE FALSE Question 10: Varimax is a form of orthogonal rotation in factor analysis TRUE FALSE "],["at-home-exercises-1.html", "2.4 At-Home Exercises", " 2.4 At-Home Exercises Coming soon to a GitBook near you! "],["in-class-exercises-1.html", "2.5 In-Class Exercises", " 2.5 In-Class Exercises Coming soon to a GitBook near you! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
